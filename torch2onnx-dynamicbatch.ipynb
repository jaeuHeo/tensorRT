{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os, torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "os.environ['NVIDIA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "\n",
    "path = '/workspace/DBP/data_storage/ocr_data/'\n",
    "os.listdir(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 utill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class CTCLabelConverter(object):\n",
    "    \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "    def __init__(self, character):\n",
    "        # character (str): set of the possible characters.\n",
    "        dict_character = list(character)\n",
    "\n",
    "        self.dict = {}\n",
    "        for i, char in enumerate(dict_character):\n",
    "            # NOTE: 0 is reserved for 'CTCblank' token required by CTCLoss\n",
    "            self.dict[char] = i + 1\n",
    "\n",
    "        self.character = ['[CTCblank]'] + dict_character  # dummy '[CTCblank]' token for CTCLoss (index 0)\n",
    "\n",
    "    def encode(self, text, batch_max_length=25):\n",
    "        \"\"\"convert text-label into text-index.\n",
    "        input:\n",
    "            text: text labels of each image. [batch_size]\n",
    "            batch_max_length: max length of text label in the batch. 25 by default\n",
    "        output:\n",
    "            text: text index for CTCLoss. [batch_size, batch_max_length]\n",
    "            length: length of each text. [batch_size]\n",
    "        \"\"\"\n",
    "        length = [len(s) for s in text]\n",
    "\n",
    "        # The index used for padding (=0) would not affect the CTC loss calculation.\n",
    "        batch_text = torch.LongTensor(len(text), batch_max_length).fill_(0)\n",
    "        for i, t in enumerate(text):\n",
    "            text = list(t)\n",
    "            text = [self.dict[char] for char in text]\n",
    "            batch_text[i][:len(text)] = torch.LongTensor(text)\n",
    "        return (batch_text.to(device), torch.IntTensor(length).to(device))\n",
    "\n",
    "    def decode(self, text_index, length):\n",
    "        \"\"\" convert text-index into text-label. \"\"\"\n",
    "        texts = []\n",
    "        for index, l in enumerate(length):\n",
    "            t = text_index[index, :]\n",
    "\n",
    "            char_list = []\n",
    "            for i in range(l):\n",
    "                if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank.\n",
    "                    char_list.append(self.character[t[i]])\n",
    "            text = ''.join(char_list)\n",
    "\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "\n",
    "\n",
    "class AttnLabelConverter(object):\n",
    "    \"\"\" Convert between text-label and text-index \"\"\"\n",
    "\n",
    "    def __init__(self, character):\n",
    "        # character (str): set of the possible characters.\n",
    "        # [GO] for the start token of the attention decoder. [s] for end-of-sentence token.\n",
    "        list_token = ['[GO]', '[s]']  # ['[s]','[UNK]','[PAD]','[GO]']\n",
    "        list_character = list(character)\n",
    "        self.character = list_token + list_character\n",
    "\n",
    "        self.dict = {}\n",
    "        for i, char in enumerate(self.character):\n",
    "            # print(i, char)\n",
    "            self.dict[char] = i\n",
    "\n",
    "    def encode(self, text, batch_max_length=25):\n",
    "        \"\"\" convert text-label into text-index.\n",
    "        input:\n",
    "            text: text labels of each image. [batch_size]\n",
    "            batch_max_length: max length of text label in the batch. 25 by default\n",
    "        output:\n",
    "            text : the input of attention decoder. [batch_size x (max_length+2)] +1 for [GO] token and +1 for [s] token.\n",
    "                text[:, 0] is [GO] token and text is padded with [GO] token after [s] token.\n",
    "            length : the length of output of attention decoder, which count [s] token also. [3, 7, ....] [batch_size]\n",
    "        \"\"\"\n",
    "        length = [len(s) + 1 for s in text]  # +1 for [s] at end of sentence.\n",
    "        # batch_max_length = max(length) # this is not allowed for multi-gpu setting\n",
    "        batch_max_length += 1\n",
    "        # additional +1 for [GO] at first step. batch_text is padded with [GO] token after [s] token.\n",
    "        batch_text = torch.LongTensor(len(text), batch_max_length + 1).fill_(0)\n",
    "        for i, t in enumerate(text):\n",
    "            text = list(t)\n",
    "            text.append('[s]')\n",
    "            text = [self.dict[char] for char in text]\n",
    "            batch_text[i][1:1 + len(text)] = torch.LongTensor(text)  # batch_text[:, 0] = [GO] token\n",
    "        return (batch_text.to(device), torch.IntTensor(length).to(device))\n",
    "\n",
    "    def decode(self, text_index, length):\n",
    "        \"\"\" convert text-index into text-label. \"\"\"\n",
    "        texts = []\n",
    "        for index, l in enumerate(length):\n",
    "            text = ''.join([self.character[i] for i in text_index[index, :]])\n",
    "            texts.append(text)\n",
    "        return texts\n",
    "\n",
    "\n",
    "class Averager(object):\n",
    "    \"\"\"Compute average for torch.Tensor, used for loss average.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def add(self, v):\n",
    "        count = v.data.numel()\n",
    "        v = v.data.sum()\n",
    "        self.n_count += count\n",
    "        self.sum += v\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_count = 0\n",
    "        self.sum = 0\n",
    "\n",
    "    def val(self):\n",
    "        res = 0\n",
    "        if self.n_count != 0:\n",
    "            res = self.sum / float(self.n_count)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import six\n",
    "import math\n",
    "import lmdb\n",
    "import torch\n",
    "\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, ConcatDataset, Subset\n",
    "from torch._utils import _accumulate\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class Batch_Balanced_Dataset(object):\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        \"\"\"\n",
    "        Modulate the data ratio in the batch.\n",
    "        For example, when select_data is \"MJ-ST\" and batch_ratio is \"0.5-0.5\",\n",
    "        the 50% of the batch is filled with MJ and the other 50% of the batch is filled with ST.\n",
    "        \"\"\"\n",
    "        log = open(f'./saved_models/{opt.exp_name}/log_dataset.txt', 'a')\n",
    "        dashed_line = '-' * 80\n",
    "        print(dashed_line)\n",
    "        log.write(dashed_line + '\\n')\n",
    "        print(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}')\n",
    "        log.write(f'dataset_root: {opt.train_data}\\nopt.select_data: {opt.select_data}\\nopt.batch_ratio: {opt.batch_ratio}\\n')\n",
    "        assert len(opt.select_data) == len(opt.batch_ratio)\n",
    "        print(opt.select_data)\n",
    "\n",
    "        _AlignCollate = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
    "        self.data_loader_list = []\n",
    "        self.dataloader_iter_list = []\n",
    "        batch_size_list = []\n",
    "        Total_batch_size = 0\n",
    "        for selected_d, batch_ratio_d in zip(opt.select_data, opt.batch_ratio):\n",
    "            _batch_size = max(round(opt.batch_size * float(batch_ratio_d)), 1)\n",
    "            print(dashed_line)\n",
    "            log.write(dashed_line + '\\n')\n",
    "            _dataset, _dataset_log = hierarchical_dataset(root=opt.train_data, opt=opt, select_data=[selected_d])\n",
    "            total_number_dataset = len(_dataset)\n",
    "            log.write(_dataset_log)\n",
    "\n",
    "            \"\"\"\n",
    "            The total number of data can be modified with opt.total_data_usage_ratio.\n",
    "            ex) opt.total_data_usage_ratio = 1 indicates 100% usage, and 0.2 indicates 20% usage.\n",
    "            See 4.2 section in our paper.\n",
    "            \"\"\"\n",
    "            number_dataset = int(total_number_dataset * float(opt.total_data_usage_ratio))\n",
    "            dataset_split = [number_dataset, total_number_dataset - number_dataset]\n",
    "            indices = range(total_number_dataset)\n",
    "            _dataset, _ = [Subset(_dataset, indices[offset - length:offset])\n",
    "                           for offset, length in zip(_accumulate(dataset_split), dataset_split)]\n",
    "            selected_d_log = f'num total samples of {selected_d}: {total_number_dataset} x {opt.total_data_usage_ratio} (total_data_usage_ratio) = {len(_dataset)}\\n'\n",
    "            selected_d_log += f'num samples of {selected_d} per batch: {opt.batch_size} x {float(batch_ratio_d)} (batch_ratio) = {_batch_size}'\n",
    "            print(selected_d_log)\n",
    "            log.write(selected_d_log + '\\n')\n",
    "            batch_size_list.append(str(_batch_size))\n",
    "            Total_batch_size += _batch_size\n",
    "\n",
    "            _data_loader = torch.utils.data.DataLoader(\n",
    "                _dataset, batch_size=_batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=int(opt.workers),\n",
    "                collate_fn=_AlignCollate, pin_memory=True)\n",
    "            self.data_loader_list.append(_data_loader)\n",
    "            self.dataloader_iter_list.append(iter(_data_loader))\n",
    "\n",
    "        Total_batch_size_log = f'{dashed_line}\\n'\n",
    "        batch_size_sum = '+'.join(batch_size_list)\n",
    "        Total_batch_size_log += f'Total_batch_size: {batch_size_sum} = {Total_batch_size}\\n'\n",
    "        Total_batch_size_log += f'{dashed_line}'\n",
    "        opt.batch_size = Total_batch_size\n",
    "\n",
    "        print(Total_batch_size_log)\n",
    "        log.write(Total_batch_size_log + '\\n')\n",
    "        log.close()\n",
    "\n",
    "    def get_batch(self):\n",
    "        balanced_batch_images = []\n",
    "        balanced_batch_texts = []\n",
    "\n",
    "        for i, data_loader_iter in enumerate(self.dataloader_iter_list):\n",
    "            try:\n",
    "                image, text = data_loader_iter.next()\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except StopIteration:\n",
    "                self.dataloader_iter_list[i] = iter(self.data_loader_list[i])\n",
    "                image, text = self.dataloader_iter_list[i].next()\n",
    "                balanced_batch_images.append(image)\n",
    "                balanced_batch_texts += text\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        balanced_batch_images = torch.cat(balanced_batch_images, 0)\n",
    "\n",
    "        return balanced_batch_images, balanced_batch_texts\n",
    "\n",
    "\n",
    "def hierarchical_dataset(root, opt, select_data='/'):\n",
    "    \"\"\" select_data='/' contains all sub-directory of root directory \"\"\"\n",
    "    dataset_list = []\n",
    "    dataset_log = f'dataset_root:    {root}\\t dataset: {select_data[0]}'\n",
    "    print(dataset_log)\n",
    "    dataset_log += '\\n'\n",
    "    for dirpath, dirnames, filenames in os.walk(root+'/'):\n",
    "        if not dirnames:\n",
    "            select_flag = False\n",
    "            for selected_d in select_data:\n",
    "                if selected_d in dirpath:\n",
    "                    select_flag = True\n",
    "                    break\n",
    "\n",
    "            if select_flag:\n",
    "                dataset = LmdbDataset(dirpath, opt)\n",
    "                sub_dataset_log = f'sub-directory:\\t/{os.path.relpath(dirpath, root)}\\t num samples: {len(dataset)}'\n",
    "                print(sub_dataset_log)\n",
    "                dataset_log += f'{sub_dataset_log}\\n'\n",
    "                dataset_list.append(dataset)\n",
    "\n",
    "    concatenated_dataset = ConcatDataset(dataset_list)\n",
    "\n",
    "    return concatenated_dataset, dataset_log\n",
    "\n",
    "\n",
    "class LmdbDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, opt):\n",
    "\n",
    "        self.root = root\n",
    "        self.opt = opt\n",
    "        self.env = lmdb.open(root, max_readers=32, readonly=True, lock=False, readahead=False, meminit=False)\n",
    "        if not self.env:\n",
    "            print('cannot create lmdb from %s' % (root))\n",
    "            sys.exit(0)\n",
    "\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            nSamples = int(txn.get('num-samples'.encode()))\n",
    "            self.nSamples = nSamples\n",
    "\n",
    "            if self.opt.data_filtering_off:\n",
    "                # for fast check or benchmark evaluation with no filtering\n",
    "                self.filtered_index_list = [index + 1 for index in range(self.nSamples)]\n",
    "            else:\n",
    "                \"\"\" Filtering part\n",
    "                If you want to evaluate IC15-2077 & CUTE datasets which have special character labels,\n",
    "                use --data_filtering_off and only evaluate on alphabets and digits.\n",
    "                see https://github.com/clovaai/deep-text-recognition-benchmark/blob/6593928855fb7abb999a99f428b3e4477d4ae356/dataset.py#L190-L192\n",
    "                And if you want to evaluate them with the model trained with --sensitive option,\n",
    "                use --sensitive and --data_filtering_off,\n",
    "                see https://github.com/clovaai/deep-text-recognition-benchmark/blob/dff844874dbe9e0ec8c5a52a7bd08c7f20afe704/test.py#L137-L144\n",
    "                \"\"\"\n",
    "                self.filtered_index_list = []\n",
    "                for index in range(self.nSamples):\n",
    "                    index += 1  # lmdb starts with 1\n",
    "                    label_key = 'label-%09d'.encode() % index\n",
    "                    label = txn.get(label_key).decode('utf-8')\n",
    "\n",
    "                    if len(label) > self.opt.batch_max_length:\n",
    "                        # print(f'The length of the label is longer than max_length: length\n",
    "                        # {len(label)}, {label} in dataset {self.root}')\n",
    "                        continue\n",
    "\n",
    "                    # By default, images containing characters which are not in opt.character are filtered.\n",
    "                    # You can add [UNK] token to `opt.character` in utils.py instead of this filtering.\n",
    "                    out_of_char = f'[^{self.opt.character}]'\n",
    "                    if re.search(out_of_char, label.lower()):\n",
    "                        continue\n",
    "\n",
    "                    self.filtered_index_list.append(index)\n",
    "\n",
    "                self.nSamples = len(self.filtered_index_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert index <= len(self), 'index range error'\n",
    "        index = self.filtered_index_list[index]\n",
    "\n",
    "        with self.env.begin(write=False) as txn:\n",
    "            label_key = 'label-%09d'.encode() % index\n",
    "            label = txn.get(label_key).decode('utf-8')\n",
    "            img_key = 'image-%09d'.encode() % index\n",
    "            imgbuf = txn.get(img_key)\n",
    "\n",
    "            buf = six.BytesIO()\n",
    "            buf.write(imgbuf)\n",
    "            buf.seek(0)\n",
    "            try:\n",
    "                if self.opt.rgb:\n",
    "                    img = Image.open(buf).convert('RGB')  # for color image\n",
    "                else:\n",
    "                    img = Image.open(buf).convert('L')\n",
    "\n",
    "            except IOError:\n",
    "                print(f'Corrupted image for {index}')\n",
    "                # make dummy image and dummy label for corrupted image.\n",
    "                if self.opt.rgb:\n",
    "                    img = Image.new('RGB', (self.opt.imgW, self.opt.imgH))\n",
    "                else:\n",
    "                    img = Image.new('L', (self.opt.imgW, self.opt.imgH))\n",
    "                label = '[dummy_label]'\n",
    "\n",
    "            if not self.opt.sensitive:\n",
    "                label = label.lower()\n",
    "\n",
    "            # We only train and evaluate on alphanumerics (or pre-defined character set in train.py)\n",
    "            out_of_char = f'[^{self.opt.character}]'\n",
    "            label = re.sub(out_of_char, '', label)\n",
    "\n",
    "        return (img, label)\n",
    "\n",
    "\n",
    "class RawDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root, opt):\n",
    "        self.opt = opt\n",
    "        self.image_path_list = []\n",
    "        for dirpath, dirnames, filenames in os.walk(root):\n",
    "            for name in filenames:\n",
    "                _, ext = os.path.splitext(name)\n",
    "                ext = ext.lower()\n",
    "                if ext == '.jpg' or ext == '.jpeg' or ext == '.png':\n",
    "                    self.image_path_list.append(os.path.join(dirpath, name))\n",
    "\n",
    "        self.image_path_list = natsorted(self.image_path_list)\n",
    "        self.nSamples = len(self.image_path_list)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        try:\n",
    "            if self.opt.rgb:\n",
    "                img = Image.open(self.image_path_list[index]).convert('RGB')  # for color image\n",
    "            else:\n",
    "                img = Image.open(self.image_path_list[index]).convert('L')\n",
    "\n",
    "        except IOError:\n",
    "            print(f'Corrupted image for {index}')\n",
    "            # make dummy image and dummy label for corrupted image.\n",
    "            if self.opt.rgb:\n",
    "                img = Image.new('RGB', (self.opt.imgW, self.opt.imgH))\n",
    "            else:\n",
    "                img = Image.new('L', (self.opt.imgW, self.opt.imgH))\n",
    "\n",
    "        return (img, self.image_path_list[index])\n",
    "\n",
    "\n",
    "class ResizeNormalize(object):\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BICUBIC):\n",
    "        self.size = size\n",
    "        self.interpolation = interpolation\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = img.resize(self.size, self.interpolation)\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        return img\n",
    "\n",
    "\n",
    "class NormalizePAD(object):\n",
    "\n",
    "    def __init__(self, max_size, PAD_type='right'):\n",
    "        self.toTensor = transforms.ToTensor()\n",
    "        self.max_size = max_size\n",
    "        self.max_width_half = math.floor(max_size[2] / 2)\n",
    "        self.PAD_type = PAD_type\n",
    "\n",
    "    def __call__(self, img):\n",
    "        img = self.toTensor(img)\n",
    "        img.sub_(0.5).div_(0.5)\n",
    "        c, h, w = img.size()\n",
    "        Pad_img = torch.FloatTensor(*self.max_size).fill_(0)\n",
    "        Pad_img[:, :, :w] = img  # right pad\n",
    "        if self.max_size[2] != w:  # add border Pad\n",
    "            Pad_img[:, :, w:] = img[:, :, w - 1].unsqueeze(2).expand(c, h, self.max_size[2] - w)\n",
    "\n",
    "        return Pad_img\n",
    "\n",
    "\n",
    "class AlignCollate(object):\n",
    "\n",
    "    def __init__(self, imgH=32, imgW=100, keep_ratio_with_pad=False):\n",
    "        self.imgH = imgH\n",
    "        self.imgW = imgW\n",
    "        self.keep_ratio_with_pad = keep_ratio_with_pad\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        batch = filter(lambda x: x is not None, batch)\n",
    "        images, labels = zip(*batch)\n",
    "\n",
    "        if self.keep_ratio_with_pad:  # same concept with 'Rosetta' paper\n",
    "            resized_max_w = self.imgW\n",
    "            input_channel = 3 if images[0].mode == 'RGB' else 1\n",
    "            transform = NormalizePAD((input_channel, self.imgH, resized_max_w))\n",
    "\n",
    "            resized_images = []\n",
    "            for image in images:\n",
    "                w, h = image.size\n",
    "                ratio = w / float(h)\n",
    "                if math.ceil(self.imgH * ratio) > self.imgW:\n",
    "                    resized_w = self.imgW\n",
    "                else:\n",
    "                    resized_w = math.ceil(self.imgH * ratio)\n",
    "\n",
    "                resized_image = image.resize((resized_w, self.imgH), Image.BICUBIC)\n",
    "                resized_images.append(transform(resized_image))\n",
    "                # resized_image.save('./image_test/%d_test.jpg' % w)\n",
    "\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in resized_images], 0)\n",
    "\n",
    "        else:\n",
    "            transform = ResizeNormalize((self.imgW, self.imgH))\n",
    "            image_tensors = [transform(image) for image in images]\n",
    "            image_tensors = torch.cat([t.unsqueeze(0) for t in image_tensors], 0)\n",
    "\n",
    "        return image_tensors, labels\n",
    "\n",
    "\n",
    "def tensor2im(image_tensor, imtype=np.uint8):\n",
    "    image_numpy = image_tensor.cpu().float().numpy()\n",
    "    if image_numpy.shape[0] == 1:\n",
    "        image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
    "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
    "    return image_numpy.astype(imtype)\n",
    "\n",
    "\n",
    "def save_image(image_numpy, image_path):\n",
    "    image_pil = Image.fromarray(image_numpy)\n",
    "    image_pil.save(image_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_FeatureExtractor(nn.Module):\n",
    "    \"\"\" FeatureExtractor of CRNN (https://arxiv.org/pdf/1507.05717.pdf) \"\"\"\n",
    "\n",
    "    def __init__(self, input_channel, output_channel=512):\n",
    "        super(VGG_FeatureExtractor, self).__init__()\n",
    "        self.output_channel = [int(output_channel / 8), int(output_channel / 4),\n",
    "                               int(output_channel / 2), output_channel]  # [64, 128, 256, 512]\n",
    "        self.ConvNet = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, self.output_channel[0], 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),  # 64x16x50\n",
    "            nn.Conv2d(self.output_channel[0], self.output_channel[1], 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),  # 128x8x25\n",
    "            nn.Conv2d(self.output_channel[1], self.output_channel[2], 3, 1, 1), nn.ReLU(True),  # 256x8x25\n",
    "            nn.Conv2d(self.output_channel[2], self.output_channel[2], 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),  # 256x4x25\n",
    "            nn.Conv2d(self.output_channel[2], self.output_channel[3], 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),  # 512x4x25\n",
    "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True),\n",
    "            nn.MaxPool2d((2, 1), (2, 1)),  # 512x2x25\n",
    "            nn.Conv2d(self.output_channel[3], self.output_channel[3], 2, 1, 0), nn.ReLU(True))  # 512x1x24\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.ConvNet(input)\n",
    "\n",
    "\n",
    "class RCNN_FeatureExtractor(nn.Module):\n",
    "    \"\"\" FeatureExtractor of GRCNN (https://papers.nips.cc/paper/6637-gated-recurrent-convolution-neural-network-for-ocr.pdf) \"\"\"\n",
    "\n",
    "    def __init__(self, input_channel, output_channel=512):\n",
    "        super(RCNN_FeatureExtractor, self).__init__()\n",
    "        self.output_channel = [int(output_channel / 8), int(output_channel / 4),\n",
    "                               int(output_channel / 2), output_channel]  # [64, 128, 256, 512]\n",
    "        self.ConvNet = nn.Sequential(\n",
    "            nn.Conv2d(input_channel, self.output_channel[0], 3, 1, 1), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),  # 64 x 16 x 50\n",
    "            GRCL(self.output_channel[0], self.output_channel[0], num_iteration=5, kernel_size=3, pad=1),\n",
    "            nn.MaxPool2d(2, 2),  # 64 x 8 x 25\n",
    "            GRCL(self.output_channel[0], self.output_channel[1], num_iteration=5, kernel_size=3, pad=1),\n",
    "            nn.MaxPool2d(2, (2, 1), (0, 1)),  # 128 x 4 x 26\n",
    "            GRCL(self.output_channel[1], self.output_channel[2], num_iteration=5, kernel_size=3, pad=1),\n",
    "            nn.MaxPool2d(2, (2, 1), (0, 1)),  # 256 x 2 x 27\n",
    "            nn.Conv2d(self.output_channel[2], self.output_channel[3], 2, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(self.output_channel[3]), nn.ReLU(True))  # 512 x 1 x 26\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.ConvNet(input)\n",
    "\n",
    "\n",
    "class ResNet_FeatureExtractor(nn.Module):\n",
    "    \"\"\" FeatureExtractor of FAN (http://openaccess.thecvf.com/content_ICCV_2017/papers/Cheng_Focusing_Attention_Towards_ICCV_2017_paper.pdf) \"\"\"\n",
    "\n",
    "    def __init__(self, input_channel, output_channel=512):\n",
    "        super(ResNet_FeatureExtractor, self).__init__()\n",
    "        self.ConvNet = ResNet(input_channel, output_channel, BasicBlock, [1, 2, 5, 3])\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.ConvNet(input)\n",
    "\n",
    "\n",
    "# For Gated RCNN\n",
    "class GRCL(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channel, output_channel, num_iteration, kernel_size, pad):\n",
    "        super(GRCL, self).__init__()\n",
    "        self.wgf_u = nn.Conv2d(input_channel, output_channel, 1, 1, 0, bias=False)\n",
    "        self.wgr_x = nn.Conv2d(output_channel, output_channel, 1, 1, 0, bias=False)\n",
    "        self.wf_u = nn.Conv2d(input_channel, output_channel, kernel_size, 1, pad, bias=False)\n",
    "        self.wr_x = nn.Conv2d(output_channel, output_channel, kernel_size, 1, pad, bias=False)\n",
    "\n",
    "        self.BN_x_init = nn.BatchNorm2d(output_channel)\n",
    "\n",
    "        self.num_iteration = num_iteration\n",
    "        self.GRCL = [GRCL_unit(output_channel) for _ in range(num_iteration)]\n",
    "        self.GRCL = nn.Sequential(*self.GRCL)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\" The input of GRCL is consistant over time t, which is denoted by u(0)\n",
    "        thus wgf_u / wf_u is also consistant over time t.\n",
    "        \"\"\"\n",
    "        wgf_u = self.wgf_u(input)\n",
    "        wf_u = self.wf_u(input)\n",
    "        x = F.relu(self.BN_x_init(wf_u))\n",
    "\n",
    "        for i in range(self.num_iteration):\n",
    "            x = self.GRCL[i](wgf_u, self.wgr_x(x), wf_u, self.wr_x(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class GRCL_unit(nn.Module):\n",
    "\n",
    "    def __init__(self, output_channel):\n",
    "        super(GRCL_unit, self).__init__()\n",
    "        self.BN_gfu = nn.BatchNorm2d(output_channel)\n",
    "        self.BN_grx = nn.BatchNorm2d(output_channel)\n",
    "        self.BN_fu = nn.BatchNorm2d(output_channel)\n",
    "        self.BN_rx = nn.BatchNorm2d(output_channel)\n",
    "        self.BN_Gx = nn.BatchNorm2d(output_channel)\n",
    "\n",
    "    def forward(self, wgf_u, wgr_x, wf_u, wr_x):\n",
    "        G_first_term = self.BN_gfu(wgf_u)\n",
    "        G_second_term = self.BN_grx(wgr_x)\n",
    "        G = F.sigmoid(G_first_term + G_second_term)\n",
    "\n",
    "        x_first_term = self.BN_fu(wf_u)\n",
    "        x_second_term = self.BN_Gx(self.BN_rx(wr_x) * G)\n",
    "        x = F.relu(x_first_term + x_second_term)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = self._conv3x3(inplanes, planes)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = self._conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def _conv3x3(self, in_planes, out_planes, stride=1):\n",
    "        \"3x3 convolution with padding\"\n",
    "        return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                         padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channel, output_channel, block, layers):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.output_channel_block = [int(output_channel / 4), int(output_channel / 2), output_channel, output_channel]\n",
    "\n",
    "        self.inplanes = int(output_channel / 8)\n",
    "        self.conv0_1 = nn.Conv2d(input_channel, int(output_channel / 16),\n",
    "                                 kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn0_1 = nn.BatchNorm2d(int(output_channel / 16))\n",
    "        self.conv0_2 = nn.Conv2d(int(output_channel / 16), self.inplanes,\n",
    "                                 kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn0_2 = nn.BatchNorm2d(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.layer1 = self._make_layer(block, self.output_channel_block[0], layers[0])\n",
    "        self.conv1 = nn.Conv2d(self.output_channel_block[0], self.output_channel_block[\n",
    "                               0], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(self.output_channel_block[0])\n",
    "\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.layer2 = self._make_layer(block, self.output_channel_block[1], layers[1], stride=1)\n",
    "        self.conv2 = nn.Conv2d(self.output_channel_block[1], self.output_channel_block[\n",
    "                               1], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(self.output_channel_block[1])\n",
    "\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=(2, 1), padding=(0, 1))\n",
    "        self.layer3 = self._make_layer(block, self.output_channel_block[2], layers[2], stride=1)\n",
    "        self.conv3 = nn.Conv2d(self.output_channel_block[2], self.output_channel_block[\n",
    "                               2], kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.output_channel_block[2])\n",
    "\n",
    "        self.layer4 = self._make_layer(block, self.output_channel_block[3], layers[3], stride=1)\n",
    "        self.conv4_1 = nn.Conv2d(self.output_channel_block[3], self.output_channel_block[\n",
    "                                 3], kernel_size=2, stride=(2, 1), padding=(0, 1), bias=False)\n",
    "        self.bn4_1 = nn.BatchNorm2d(self.output_channel_block[3])\n",
    "        self.conv4_2 = nn.Conv2d(self.output_channel_block[3], self.output_channel_block[\n",
    "                                 3], kernel_size=2, stride=1, padding=0, bias=False)\n",
    "        self.bn4_2 = nn.BatchNorm2d(self.output_channel_block[3])\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0_1(x)\n",
    "        x = self.bn0_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv0_2(x)\n",
    "        x = self.bn0_2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.bn4_1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv4_2(x)\n",
    "        x = self.bn4_2(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_cell = AttentionCell(input_size, hidden_size, num_classes)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.generator = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def _char_to_onehot(self, input_char, onehot_dim=38):\n",
    "        input_char = input_char.unsqueeze(1)\n",
    "        batch_size = input_char.size(0)\n",
    "        one_hot = torch.FloatTensor(batch_size, onehot_dim).zero_().to(device)\n",
    "        one_hot = one_hot.scatter_(1, input_char, 1)\n",
    "        return one_hot\n",
    "\n",
    "    def forward(self, batch_H, text, is_train=True, batch_max_length=25):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            batch_H : contextual_feature H = hidden state of encoder. [batch_size x num_steps x contextual_feature_channels]\n",
    "            text : the text-index of each image. [batch_size x (max_length+1)]. +1 for [GO] token. text[:, 0] = [GO].\n",
    "        output: probability distribution at each step [batch_size x num_steps x num_classes]\n",
    "        \"\"\"\n",
    "        batch_size = batch_H.size(0)\n",
    "        num_steps = batch_max_length + 1  # +1 for [s] at end of sentence.\n",
    "\n",
    "        output_hiddens = torch.FloatTensor(batch_size, num_steps, self.hidden_size).fill_(0).to(device)\n",
    "        hidden = (torch.FloatTensor(batch_size, self.hidden_size).fill_(0).to(device),\n",
    "                  torch.FloatTensor(batch_size, self.hidden_size).fill_(0).to(device))\n",
    "\n",
    "        if is_train:\n",
    "            for i in range(num_steps):\n",
    "                # one-hot vectors for a i-th char. in a batch\n",
    "                char_onehots = self._char_to_onehot(text[:, i], onehot_dim=self.num_classes)\n",
    "                # hidden : decoder's hidden s_{t-1}, batch_H : encoder's hidden H, char_onehots : one-hot(y_{t-1})\n",
    "                hidden, alpha = self.attention_cell(hidden, batch_H, char_onehots)\n",
    "                output_hiddens[:, i, :] = hidden[0]  # LSTM hidden index (0: hidden, 1: Cell)\n",
    "            probs = self.generator(output_hiddens)\n",
    "\n",
    "        else:\n",
    "            targets = torch.LongTensor(batch_size).fill_(0).to(device)  # [GO] token\n",
    "            probs = torch.FloatTensor(batch_size, num_steps, self.num_classes).fill_(0).to(device)\n",
    "\n",
    "            for i in range(num_steps):\n",
    "                char_onehots = self._char_to_onehot(targets, onehot_dim=self.num_classes)\n",
    "                hidden, alpha = self.attention_cell(hidden, batch_H, char_onehots)\n",
    "                probs_step = self.generator(hidden[0])\n",
    "                probs[:, i, :] = probs_step\n",
    "                _, next_input = probs_step.max(1)\n",
    "                targets = next_input\n",
    "\n",
    "        return probs  # batch_size x num_steps x num_classes\n",
    "\n",
    "\n",
    "class AttentionCell(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, num_embeddings):\n",
    "        super(AttentionCell, self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)  # either i2i or h2h should have bias\n",
    "        self.score = nn.Linear(hidden_size, 1, bias=False)\n",
    "        self.rnn = nn.LSTMCell(input_size + num_embeddings, hidden_size)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, prev_hidden, batch_H, char_onehots):\n",
    "        # [batch_size x num_encoder_step x num_channel] -> [batch_size x num_encoder_step x hidden_size]\n",
    "        batch_H_proj = self.i2h(batch_H)\n",
    "        prev_hidden_proj = self.h2h(prev_hidden[0]).unsqueeze(1)\n",
    "        e = self.score(torch.tanh(batch_H_proj + prev_hidden_proj))  # batch_size x num_encoder_step * 1\n",
    "\n",
    "        alpha = F.softmax(e, dim=1)\n",
    "        context = torch.bmm(alpha.permute(0, 2, 1), batch_H).squeeze(1)  # batch_size x num_channel\n",
    "        concat_context = torch.cat([context, char_onehots], 1)  # batch_size x (num_channel + num_embedding)\n",
    "        cur_hidden = self.rnn(concat_context, prev_hidden)\n",
    "        return cur_hidden, alpha\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        input : visual feature [batch_size x T x input_size]\n",
    "        output : contextual feature [batch_size x T x output_size]\n",
    "        \"\"\"\n",
    "        self.rnn.flatten_parameters()\n",
    "        recurrent, _ = self.rnn(input)  # batch_size x T x input_size -> batch_size x T x (2*hidden_size)\n",
    "        output = self.linear(recurrent)  # batch_size x T x output_size\n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPS_SpatialTransformerNetwork(nn.Module):\n",
    "    \"\"\" Rectification Network of RARE, namely TPS based STN \"\"\"\n",
    "\n",
    "    def __init__(self, F, I_size, I_r_size, I_channel_num=1):\n",
    "        \"\"\" Based on RARE TPS\n",
    "        input:\n",
    "            batch_I: Batch Input Image [batch_size x I_channel_num x I_height x I_width]\n",
    "            I_size : (height, width) of the input image I\n",
    "            I_r_size : (height, width) of the rectified image I_r\n",
    "            I_channel_num : the number of channels of the input image I\n",
    "        output:\n",
    "            batch_I_r: rectified image [batch_size x I_channel_num x I_r_height x I_r_width]\n",
    "        \"\"\"\n",
    "        super(TPS_SpatialTransformerNetwork, self).__init__()\n",
    "        self.F = F\n",
    "        self.I_size = I_size\n",
    "        self.I_r_size = I_r_size  # = (I_r_height, I_r_width)\n",
    "        self.I_channel_num = I_channel_num\n",
    "        self.LocalizationNetwork = LocalizationNetwork(self.F, self.I_channel_num)\n",
    "        self.GridGenerator = GridGenerator(self.F, self.I_r_size)\n",
    "\n",
    "    def forward(self, batch_I):\n",
    "        batch_C_prime = self.LocalizationNetwork(batch_I)  # batch_size x K x 2\n",
    "        build_P_prime = self.GridGenerator.build_P_prime(batch_C_prime)  # batch_size x n (= I_r_width x I_r_height) x 2\n",
    "        build_P_prime_reshape = build_P_prime.reshape([build_P_prime.size(0), self.I_r_size[0], self.I_r_size[1], 2])\n",
    "        \n",
    "        if torch.__version__ > \"1.2.0\":\n",
    "            batch_I_r = F.grid_sample(batch_I, build_P_prime_reshape, padding_mode='border', align_corners=True)\n",
    "        else:\n",
    "            batch_I_r = F.grid_sample(batch_I, build_P_prime_reshape, padding_mode='border')\n",
    "\n",
    "        return batch_I_r\n",
    "\n",
    "\n",
    "class LocalizationNetwork(nn.Module):\n",
    "    \"\"\" Localization Network of RARE, which predicts C' (K x 2) from I (I_width x I_height) \"\"\"\n",
    "\n",
    "    def __init__(self, F, I_channel_num):\n",
    "        super(LocalizationNetwork, self).__init__()\n",
    "        self.F = F\n",
    "        self.I_channel_num = I_channel_num\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.I_channel_num, out_channels=64, kernel_size=3, stride=1, padding=1,\n",
    "                      bias=False), nn.BatchNorm2d(64), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),  # batch_size x 64 x I_height/2 x I_width/2\n",
    "            nn.Conv2d(64, 128, 3, 1, 1, bias=False), nn.BatchNorm2d(128), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),  # batch_size x 128 x I_height/4 x I_width/4\n",
    "            nn.Conv2d(128, 256, 3, 1, 1, bias=False), nn.BatchNorm2d(256), nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),  # batch_size x 256 x I_height/8 x I_width/8\n",
    "            nn.Conv2d(256, 512, 3, 1, 1, bias=False), nn.BatchNorm2d(512), nn.ReLU(True),\n",
    "            nn.AdaptiveAvgPool2d(1)  # batch_size x 512\n",
    "        )\n",
    "\n",
    "        self.localization_fc1 = nn.Sequential(nn.Linear(512, 256), nn.ReLU(True))\n",
    "        self.localization_fc2 = nn.Linear(256, self.F * 2)\n",
    "\n",
    "        # Init fc2 in LocalizationNetwork\n",
    "        self.localization_fc2.weight.data.fill_(0)\n",
    "        \"\"\" see RARE paper Fig. 6 (a) \"\"\"\n",
    "        ctrl_pts_x = np.linspace(-1.0, 1.0, int(F / 2))\n",
    "        ctrl_pts_y_top = np.linspace(0.0, -1.0, num=int(F / 2))\n",
    "        ctrl_pts_y_bottom = np.linspace(1.0, 0.0, num=int(F / 2))\n",
    "        ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n",
    "        ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n",
    "        initial_bias = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n",
    "        self.localization_fc2.bias.data = torch.from_numpy(initial_bias).float().view(-1)\n",
    "\n",
    "    def forward(self, batch_I):\n",
    "        \"\"\"\n",
    "        input:     batch_I : Batch Input Image [batch_size x I_channel_num x I_height x I_width]\n",
    "        output:    batch_C_prime : Predicted coordinates of fiducial points for input batch [batch_size x F x 2]\n",
    "        \"\"\"\n",
    "        batch_size = batch_I.size(0)\n",
    "        features = self.conv(batch_I).view(batch_size, -1)\n",
    "        batch_C_prime = self.localization_fc2(self.localization_fc1(features)).view(batch_size, self.F, 2)\n",
    "        return batch_C_prime\n",
    "\n",
    "\n",
    "class GridGenerator(nn.Module):\n",
    "    \"\"\" Grid Generator of RARE, which produces P_prime by multipling T with P \"\"\"\n",
    "\n",
    "    def __init__(self, F, I_r_size):\n",
    "        \"\"\" Generate P_hat and inv_delta_C for later \"\"\"\n",
    "        super(GridGenerator, self).__init__()\n",
    "        self.eps = 1e-6\n",
    "        self.I_r_height, self.I_r_width = I_r_size\n",
    "        self.F = F\n",
    "        self.C = self._build_C(self.F)  # F x 2\n",
    "        self.P = self._build_P(self.I_r_width, self.I_r_height)\n",
    "        ## for multi-gpu, you need register buffer\n",
    "        self.register_buffer(\"inv_delta_C\", torch.tensor(self._build_inv_delta_C(self.F, self.C)).float())  # F+3 x F+3\n",
    "        self.register_buffer(\"P_hat\", torch.tensor(self._build_P_hat(self.F, self.C, self.P)).float())  # n x F+3\n",
    "        ## for fine-tuning with different image width, you may use below instead of self.register_buffer\n",
    "        #self.inv_delta_C = torch.tensor(self._build_inv_delta_C(self.F, self.C)).float().cuda()  # F+3 x F+3\n",
    "        #self.P_hat = torch.tensor(self._build_P_hat(self.F, self.C, self.P)).float().cuda()  # n x F+3\n",
    "\n",
    "    def _build_C(self, F):\n",
    "        \"\"\" Return coordinates of fiducial points in I_r; C \"\"\"\n",
    "        ctrl_pts_x = np.linspace(-1.0, 1.0, int(F / 2))\n",
    "        ctrl_pts_y_top = -1 * np.ones(int(F / 2))\n",
    "        ctrl_pts_y_bottom = np.ones(int(F / 2))\n",
    "        ctrl_pts_top = np.stack([ctrl_pts_x, ctrl_pts_y_top], axis=1)\n",
    "        ctrl_pts_bottom = np.stack([ctrl_pts_x, ctrl_pts_y_bottom], axis=1)\n",
    "        C = np.concatenate([ctrl_pts_top, ctrl_pts_bottom], axis=0)\n",
    "        return C  # F x 2\n",
    "\n",
    "    def _build_inv_delta_C(self, F, C):\n",
    "        \"\"\" Return inv_delta_C which is needed to calculate T \"\"\"\n",
    "        hat_C = np.zeros((F, F), dtype=float)  # F x F\n",
    "        for i in range(0, F):\n",
    "            for j in range(i, F):\n",
    "                r = np.linalg.norm(C[i] - C[j])\n",
    "                hat_C[i, j] = r\n",
    "                hat_C[j, i] = r\n",
    "        np.fill_diagonal(hat_C, 1)\n",
    "        hat_C = (hat_C ** 2) * np.log(hat_C)\n",
    "        # print(C.shape, hat_C.shape)\n",
    "        delta_C = np.concatenate(  # F+3 x F+3\n",
    "            [\n",
    "                np.concatenate([np.ones((F, 1)), C, hat_C], axis=1),  # F x F+3\n",
    "                np.concatenate([np.zeros((2, 3)), np.transpose(C)], axis=1),  # 2 x F+3\n",
    "                np.concatenate([np.zeros((1, 3)), np.ones((1, F))], axis=1)  # 1 x F+3\n",
    "            ],\n",
    "            axis=0\n",
    "        )\n",
    "        inv_delta_C = np.linalg.inv(delta_C)\n",
    "        return inv_delta_C  # F+3 x F+3\n",
    "\n",
    "    def _build_P(self, I_r_width, I_r_height):\n",
    "        I_r_grid_x = (np.arange(-I_r_width, I_r_width, 2) + 1.0) / I_r_width  # self.I_r_width\n",
    "        I_r_grid_y = (np.arange(-I_r_height, I_r_height, 2) + 1.0) / I_r_height  # self.I_r_height\n",
    "        P = np.stack(  # self.I_r_width x self.I_r_height x 2\n",
    "            np.meshgrid(I_r_grid_x, I_r_grid_y),\n",
    "            axis=2\n",
    "        )\n",
    "        return P.reshape([-1, 2])  # n (= self.I_r_width x self.I_r_height) x 2\n",
    "\n",
    "    def _build_P_hat(self, F, C, P):\n",
    "        n = P.shape[0]  # n (= self.I_r_width x self.I_r_height)\n",
    "        P_tile = np.tile(np.expand_dims(P, axis=1), (1, F, 1))  # n x 2 -> n x 1 x 2 -> n x F x 2\n",
    "        C_tile = np.expand_dims(C, axis=0)  # 1 x F x 2\n",
    "        P_diff = P_tile - C_tile  # n x F x 2\n",
    "        rbf_norm = np.linalg.norm(P_diff, ord=2, axis=2, keepdims=False)  # n x F\n",
    "        rbf = np.multiply(np.square(rbf_norm), np.log(rbf_norm + self.eps))  # n x F\n",
    "        P_hat = np.concatenate([np.ones((n, 1)), P, rbf], axis=1)\n",
    "        return P_hat  # n x F+3\n",
    "\n",
    "    def build_P_prime(self, batch_C_prime):\n",
    "        \"\"\" Generate Grid from batch_C_prime [batch_size x F x 2] \"\"\"\n",
    "        batch_size = batch_C_prime.size(0)\n",
    "        batch_inv_delta_C = self.inv_delta_C.repeat(batch_size, 1, 1)\n",
    "        batch_P_hat = self.P_hat.repeat(batch_size, 1, 1)\n",
    "        batch_C_prime_with_zeros = torch.cat((batch_C_prime, torch.zeros(\n",
    "            batch_size, 3, 2).float().to(device)), dim=1)  # batch_size x F+3 x 2\n",
    "        batch_T = torch.bmm(batch_inv_delta_C, batch_C_prime_with_zeros)  # batch_size x F+3 x 2\n",
    "        batch_P_prime = torch.bmm(batch_P_hat, batch_T)  # batch_size x n x 2\n",
    "        return batch_P_prime  # batch_size x n x 2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, opt):\n",
    "        super(Model, self).__init__()\n",
    "        self.opt = opt\n",
    "        self.stages = {'Trans': opt.Transformation, 'Feat': opt.FeatureExtraction,\n",
    "                       'Seq': opt.SequenceModeling, 'Pred': opt.Prediction}\n",
    "\n",
    "        \"\"\" Transformation \"\"\"\n",
    "        if opt.Transformation == 'TPS':\n",
    "            self.Transformation = TPS_SpatialTransformerNetwork(\n",
    "                F=opt.num_fiducial, I_size=(opt.imgH, opt.imgW), I_r_size=(opt.imgH, opt.imgW), I_channel_num=opt.input_channel)\n",
    "        else:\n",
    "            print('No Transformation module specified')\n",
    "\n",
    "        \"\"\" FeatureExtraction \"\"\"\n",
    "        if opt.FeatureExtraction == 'VGG':\n",
    "            self.FeatureExtraction = VGG_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
    "        elif opt.FeatureExtraction == 'RCNN':\n",
    "            self.FeatureExtraction = RCNN_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
    "        elif opt.FeatureExtraction == 'ResNet':\n",
    "            self.FeatureExtraction = ResNet_FeatureExtractor(opt.input_channel, opt.output_channel)\n",
    "        else:\n",
    "            raise Exception('No FeatureExtraction module specified')\n",
    "        self.FeatureExtraction_output = opt.output_channel  # int(imgH/16-1) * 512\n",
    "        self.AdaptiveAvgPool = nn.AdaptiveAvgPool2d((None, 1))  # Transform final (imgH/16-1) -> 1\n",
    "\n",
    "        \"\"\" Sequence modeling\"\"\"\n",
    "        if opt.SequenceModeling == 'BiLSTM':\n",
    "            self.SequenceModeling = nn.Sequential(\n",
    "                BidirectionalLSTM(self.FeatureExtraction_output, opt.hidden_size, opt.hidden_size),\n",
    "                BidirectionalLSTM(opt.hidden_size, opt.hidden_size, opt.hidden_size))\n",
    "            self.SequenceModeling_output = opt.hidden_size\n",
    "        else:\n",
    "            print('No SequenceModeling module specified')\n",
    "            self.SequenceModeling_output = self.FeatureExtraction_output\n",
    "\n",
    "        \"\"\" Prediction \"\"\"\n",
    "        if opt.Prediction == 'CTC':\n",
    "            self.Prediction = nn.Linear(self.SequenceModeling_output, opt.num_class)\n",
    "        elif opt.Prediction == 'Attn':\n",
    "            self.Prediction = Attention(self.SequenceModeling_output, opt.hidden_size, opt.num_class)\n",
    "        else:\n",
    "            raise Exception('Prediction is neither CTC or Attn')\n",
    "\n",
    "    def forward(self, input):\n",
    "        is_train=False\n",
    "        \"\"\" Transformation stage \"\"\"\n",
    "        if not self.stages['Trans'] == \"None\":\n",
    "            input = self.Transformation(input)\n",
    "\n",
    "        \"\"\" Feature extraction stage \"\"\"\n",
    "        visual_feature = self.FeatureExtraction(input)\n",
    "        visual_feature = self.AdaptiveAvgPool(visual_feature.permute(0, 3, 1, 2))  # [b, c, h, w] -> [b, w, c, h]\n",
    "        visual_feature = visual_feature.squeeze(3)\n",
    "\n",
    "        \"\"\" Sequence modeling stage \"\"\"\n",
    "        if self.stages['Seq'] == 'BiLSTM':\n",
    "            contextual_feature = self.SequenceModeling(visual_feature)\n",
    "        else:\n",
    "            contextual_feature = visual_feature  # for convenience. this is NOT contextually modeled by BiLSTM\n",
    "        \n",
    "        batch_size= input.shape[0]\n",
    "        text = torch.LongTensor(batch_size, self.opt.batch_max_length + 1).fill_(0).to(device)\n",
    "        \"\"\" Prediction stage \"\"\"\n",
    "        if self.stages['Pred'] == 'CTC':\n",
    "            prediction = self.Prediction(contextual_feature.contiguous())\n",
    "        else:\n",
    "            prediction = self.Prediction(contextual_feature.contiguous(), text, is_train, batch_max_length=self.opt.batch_max_length)\n",
    "\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def img_show(path, size =(15,15)):\n",
    "    img = cv2.imread(path, cv2.IMREAD_UNCHANGED)\n",
    "    plt.rcParams[\"figure.figsize\"] = size\n",
    "    imgplot = plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def demo(opt):\n",
    "    \"\"\" model configuration \"\"\"\n",
    "    if 'CTC' in opt.Prediction:\n",
    "        converter = CTCLabelConverter(opt.character)\n",
    "    else:\n",
    "        converter = AttnLabelConverter(opt.character)\n",
    "    opt.num_class = len(converter.character)\n",
    "    print(opt.num_class)\n",
    "    if opt.rgb:\n",
    "        opt.input_channel = 3\n",
    "    model = Model(opt)\n",
    "    print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "          opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "          opt.SequenceModeling, opt.Prediction)\n",
    "    print(device)\n",
    "#     model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "    # load model\n",
    "    print('loading pretrained model from %s' % opt.saved_model)\n",
    "    model.load_state_dict(torch.load(opt.saved_model, map_location=device))\n",
    "    model = model.to(device)\n",
    "    # prepare data. two demo images from https://github.com/bgshih/crnn#run-demo\n",
    "    AlignCollate_demo = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
    "    demo_data = RawDataset(root=opt.image_folder, opt=opt)  # use RawDataset\n",
    "    demo_loader = torch.utils.data.DataLoader(\n",
    "        demo_data, batch_size=opt.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=int(opt.workers),\n",
    "        collate_fn=AlignCollate_demo, pin_memory=True)\n",
    "    print('데이터 생성완료')\n",
    "    # predict\n",
    "    model.eval()\n",
    "    result_list = []\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for image_tensors, image_path_list in demo_loader:\n",
    "            \n",
    "            batch_size = image_tensors.size(0)\n",
    "            image = image_tensors.to(device)\n",
    "            # For max length prediction\n",
    "            length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "            text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "            print('데이터 텐서화')\n",
    "\n",
    "            if 'CTC' in opt.Prediction:\n",
    "                preds = model(image)\n",
    "                print(preds.shape)\n",
    "                # Select max probabilty (greedy decoding) then decode index to character\n",
    "                preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "                _, preds_index = preds.max(2)\n",
    "                preds_index = preds_index.view(-1)\n",
    "                preds_str = converter.decode(preds_index.data, preds_size.data)\n",
    "                \n",
    "\n",
    "            else:\n",
    "                print('182')\n",
    "                preds = model(image)\n",
    "                \n",
    "\n",
    "                # select max probabilty (greedy decoding) then decode index to character\n",
    "                _, preds_index = preds.max(2)\n",
    "                preds_str = converter.decode(preds_index, length_for_pred)\n",
    "                return model, image, text_for_pred, preds, preds_index, preds_str\n",
    "            \n",
    "\n",
    "\n",
    "            log = open(f'./saved_models/log_demo_result_douzone16_i.txt', 'a')\n",
    "            dashed_line = '-' * 80\n",
    "            head = f'{\"image_path\":25s}\\t{\"predicted_labels\":25s}\\tconfidence score'\n",
    "            \n",
    "            print(f'{dashed_line}\\n{head}\\n{dashed_line}')\n",
    "            log.write(f'{dashed_line}\\n{head}\\n{dashed_line}\\n')\n",
    "\n",
    "            preds_prob = F.softmax(preds, dim=2)\n",
    "            preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "            count = 0\n",
    "            \n",
    "            for img_name, pred, pred_max_prob in zip(image_path_list, preds_str, preds_max_prob):\n",
    "                count +=1\n",
    "                if 'Attn' in opt.Prediction:\n",
    "                    pred_EOS = pred.find('[s]')\n",
    "                    pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "                    pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "                # calculate confidence score (= multiply of pred_max_prob)\n",
    "                confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "                result_list.append([img_name,pred,confidence_score])\n",
    "                print('183')\n",
    "                print(f'{img_name:25s}\\t{pred:25s}\\t{confidence_score:0.4f}')\n",
    "                log.write(f'{img_name:25s}\\t{pred:25s}\\t{confidence_score:0.4f}\\n')\n",
    "#                 img_show(img_name)\n",
    "#                 print(pred,confidence_score)\n",
    "\n",
    "            log.close()\n",
    "    return result_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# \"\"\"word/sentence\"\"\"\n",
    "# target = 'word'\n",
    "\n",
    "# def get_img_id(path, get_file_list = ['train', 'val', 'test']):\n",
    "#     return_dict = {}\n",
    "#     for name in get_file_list:\n",
    "#         keys = pd.read_csv(path+f'/{name}_gt.txt',delimiter='\\t',names=['path','data']).path.apply(lambda x: x.split('/')[-1].split('.')[0]).tolist()\n",
    "#         return_dict[name] = keys\n",
    "#     return return_dict\n",
    "\n",
    "# return_dict = get_img_id(path+target)\n",
    "\n",
    "# width_list, height_list, text_len_list, text = [], [], [], ''\n",
    "# for key in ['train','val']:\n",
    "#     keys = return_dict[key]\n",
    "#     for i in keys:\n",
    "#         width_list.append(data_info_dict[i]['info']['width'])\n",
    "#         height_list.append(data_info_dict[i]['info']['height'])\n",
    "#         text += data_info_dict[i]['data']['text']\n",
    "#         text_len_list.append(len(data_info_dict[i]['data']['text']))\n",
    "        \n",
    "\n",
    "# character = np.unique([i for i in text]).tolist()\n",
    "# character = ''.join(character)\n",
    "# width_mean, height_mean = np.mean(width_list), np.mean(height_list)\n",
    "# print(f\"\"\"mean_width: {width_mean}\\nmean_height: {height_mean}\\nmax_sentence_len: {np.max(text_len_list)}\\ncharacter: {character}\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def add_argument(self, key, default=None, help=None,action=None,required=False,type=None):\n",
    "        key = key.replace('-','')\n",
    "        if action == 'store_true':\n",
    "            self.__dict__[key] = False\n",
    "        else:\n",
    "            self.__dict__[key] = default\n",
    "            \n",
    "        if required:\n",
    "            print(key,'/',help)\n",
    "            \n",
    "            \n",
    "    def set_argument(self, data_dict):\n",
    "        for key in data_dict:\n",
    "            self.__dict__[key] = data_dict[key]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Args()\n",
    "\n",
    "parser.add_argument('--image_folder', required=True, help='path to image_folder which contains text images')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=4)\n",
    "parser.add_argument('--batch_size', type=int, default=192, help='input batch size')\n",
    "parser.add_argument('--saved_model', required=True, help=\"path to saved_model to evaluation\")\n",
    "\"\"\" Data processing \"\"\"\n",
    "parser.add_argument('--batch_max_length', type=int, default=25, help='maximum-label-length')\n",
    "parser.add_argument('--imgH', type=int, default=32, help='the height of the input image')\n",
    "parser.add_argument('--imgW', type=int, default=100, help='the width of the input image')\n",
    "parser.add_argument('--rgb', action='store_true', help='use rgb input')\n",
    "parser.add_argument('--character', type=str, default='0123456789abcdefghijklmnopqrstuvwxyz', help='character label')\n",
    "parser.add_argument('--sensitive', default=False,action='store_true', help='for sensitive character mode')\n",
    "parser.add_argument('--PAD', action='store_true', help='whether to keep ratio then pad for image resize')\n",
    "\"\"\" Model Architecture \"\"\"\n",
    "parser.add_argument('--Transformation', type=str, required=True, help='Transformation stage. None|TPS')\n",
    "parser.add_argument('--FeatureExtraction', type=str, required=True, help='FeatureExtraction stage. VGG|RCNN|ResNet')\n",
    "parser.add_argument('--SequenceModeling', type=str, required=True, help='SequenceModeling stage. None|BiLSTM')\n",
    "parser.add_argument('--Prediction', type=str, required=True, help='Prediction stage. CTC|Attn')\n",
    "parser.add_argument('--num_fiducial', type=int, default=20, help='number of fiducial points of TPS-STN')\n",
    "parser.add_argument('--input_channel', type=int, default=1, help='the number of input channel of Feature extractor')\n",
    "parser.add_argument('--output_channel', type=int, default=512,\n",
    "                    help='the number of output channel of Feature extractor')\n",
    "parser.add_argument('--hidden_size', type=int, default=256, help='the size of the LSTM hidden state')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 실행용 파라미터\n",
    "parser = Args()\n",
    "\n",
    "parser.add_argument('--image_folder', required=True, help='path to image_folder which contains text images')\n",
    "parser.add_argument('--eval_data', required=True, help='path to evaluation dataset')\n",
    "parser.add_argument('--benchmark_all_eval', action='store_true', help='evaluate 10 benchmark evaluation datasets')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=4)\n",
    "parser.add_argument('--batch_size', type=int, default=192, help='input batch size')\n",
    "parser.add_argument('--saved_model', required=True, help=\"path to saved_model to evaluation\")\n",
    "\"\"\" Data processing \"\"\"\n",
    "parser.add_argument('--batch_max_length', type=int, default=25, help='maximum-label-length')\n",
    "parser.add_argument('--imgH', type=int, default=32, help='the height of the input image')\n",
    "parser.add_argument('--imgW', type=int, default=100, help='the width of the input image')\n",
    "parser.add_argument('--rgb', action='store_true', help='use rgb input')\n",
    "parser.add_argument('--character', type=str, default='0123456789abcdefghijklmnopqrstuvwxyz', help='character label')\n",
    "parser.add_argument('--sensitive', default=False,action='store_true', help='for sensitive character mode')\n",
    "parser.add_argument('--PAD', action='store_true', help='whether to keep ratio then pad for image resize')\n",
    "\"\"\" Model Architecture \"\"\"\n",
    "parser.add_argument('--Transformation', type=str, required=True, help='Transformation stage. None|TPS')\n",
    "parser.add_argument('--FeatureExtraction', type=str, required=True, help='FeatureExtraction stage. VGG|RCNN|ResNet')\n",
    "parser.add_argument('--SequenceModeling', type=str, required=True, help='SequenceModeling stage. None|BiLSTM')\n",
    "parser.add_argument('--Prediction', type=str, required=True, help='Prediction stage. CTC|Attn')\n",
    "parser.add_argument('--num_fiducial', type=int, default=20, help='number of fiducial points of TPS-STN')\n",
    "parser.add_argument('--input_channel', type=int, default=1, help='the number of input channel of Feature extractor')\n",
    "parser.add_argument('--output_channel', type=int, default=512,\n",
    "                    help='the number of output channel of Feature extractor')\n",
    "parser.add_argument('--hidden_size', type=int, default=256, help='the size of the LSTM hidden state')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './'\n",
    "# model_path = '/workspace/DBP/서류ocr인식/saved_models/TPS-ResNet-BiLSTM-Attn-Seed1119/'\n",
    "save_model_name = 'best_accuracy_noparell.pth'\n",
    "\n",
    "# save_model_name = \"None-VGG-None-CTC.pth\"\n",
    "# save_model_name = 'best_norm_ED.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyStateDict(state_dict):\n",
    "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
    "        start_idx = 1\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = \".\".join(k.split(\".\")[start_idx:])\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_path+'opt.txt','r') as f:\n",
    "    opt = f.read()\n",
    "    \n",
    "opt = opt.split('------------ Options -------------\\n')\n",
    "opt = opt[-1].split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_dict ={}\n",
    "int_keys = ['manualSeed', 'workers', 'batch_size', 'num_iter', 'valInterval', 'batch_max_length', 'imgH', 'imgW', 'num_fiducial', 'input_channel', 'output_channel', 'hidden_size']\n",
    "float_keys = ['lr', 'beta1', 'rho', 'eps', 'grad_clip']\n",
    "bool_keys = ['FT', 'adam', 'rgb', 'sensitive', 'PAD', 'data_filtering_off']\n",
    "str_keys = ['exp_name', 'train_data', 'valid_data', 'saved_model', 'select_data', 'batch_ratio', 'total_data_usage_ratio', 'character', 'Transformation', 'FeatureExtraction', 'SequenceModeling', 'Prediction']\n",
    "save_keys = ['image_folder', 'workers', 'batch_size', 'saved_model', 'batch_max_length', 'imgH', 'imgW', 'rgb', 'character', 'sensitive', 'PAD', 'Transformation', 'FeatureExtraction', 'SequenceModeling', 'Prediction', 'num_fiducial', 'input_channel', 'output_channel', 'hidden_size','rgb']\n",
    "for i in opt:\n",
    "    t = i.split(':')\n",
    "    if i == '---------------------------------------':\n",
    "        break\n",
    "    else:\n",
    "        key, data = t[0].strip(), t[1][1:]\n",
    "        if key in int_keys:\n",
    "            data = int(data)\n",
    "        elif key in float_keys:\n",
    "            data = float(data)\n",
    "        elif key in bool_keys:\n",
    "            if data == 'True':\n",
    "                data = True\n",
    "            elif data == 'False':\n",
    "                data = False      \n",
    "        elif key in str_keys:\n",
    "            data = str(data)\n",
    "        if key in save_keys:\n",
    "            opt_dict[key] = data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test 실행용\n",
    "opt_dict ={}\n",
    "int_keys = ['manualSeed', 'workers', 'batch_size', 'num_iter', 'valInterval', 'batch_max_length', 'imgH', 'imgW', 'num_fiducial', 'input_channel', 'output_channel', 'hidden_size']\n",
    "float_keys = ['lr', 'beta1', 'rho', 'eps', 'grad_clip']\n",
    "bool_keys = ['FT', 'adam', 'rgb', 'sensitive', 'PAD', 'data_filtering_off']\n",
    "str_keys = ['exp_name', 'train_data', 'valid_data', 'saved_model', 'select_data', 'batch_ratio', 'total_data_usage_ratio', 'character', 'Transformation', 'FeatureExtraction', 'SequenceModeling', 'Prediction']\n",
    "save_keys = ['image_folder', 'workers', 'batch_size', 'saved_model', 'batch_max_length', 'imgH', 'imgW', 'rgb', 'character', 'sensitive', 'PAD', 'Transformation', 'FeatureExtraction', 'SequenceModeling', 'Prediction', 'num_fiducial', 'input_channel', 'output_channel', 'hidden_size','rgb']\n",
    "for i in opt:\n",
    "    t = i.split(':')\n",
    "    if i == '---------------------------------------':\n",
    "        break\n",
    "    else:\n",
    "        key, data = t[0].strip(), t[1][1:]\n",
    "        if key in int_keys:\n",
    "            data = int(data)\n",
    "        elif key in float_keys:\n",
    "            data = float(data)\n",
    "        elif key in bool_keys:\n",
    "            if data == 'True':\n",
    "                data = True\n",
    "            elif data == 'False':\n",
    "                data = False      \n",
    "        elif key in str_keys:\n",
    "            data = str(data)\n",
    "        if key in save_keys:\n",
    "            opt_dict[key] = data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_folder = '/workspace/DBP/data_storage/ocr_data/ocr_test_data/doc/'\n",
    "# image_folder = '/workspace/DBP/data_storage/ocr_data/ocr_test_data/douzonepdf_test/cut_result2/'\n",
    "image_folder = '/workspace/DBP/NAS저장공간/hengbee/torch-onnx-trt-Inference-CRAFT/deep-text-recognition-benchmark/demo_image'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser.set_argument(opt_dict)\n",
    "# parser.image_folder = image_folder\n",
    "# parser.saved_model = model_path+save_model_name\n",
    "\n",
    "\n",
    "# opt = parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.set_argument(opt_dict)\n",
    "parser.sensitive = True\n",
    "parser.character = ' !\"%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\_abcdefghijklmnopqrstuvwxyz{}~→ㄱㄴㄹㅁㅅㅇㅔㅠㅣ가각간갇갈감갑값갓갔강갖같갚갛개객갤갬갯갱걀걔거걱건걷걸검겁것겅겉게겐겔겠겨격겪견결겸겹겼경곁계고곡곤곧골곰곱곳공곶과곽관괄괌광괘괜괭괴굉교구국군굳굴굵굶굽굿궁권궐궤귀귄귓규균귤그극근글긁금급긋긍기긴길김깁깃깅깆깊까깍깎깐깔깜깝깡깥깨깬깽꺠꺵꺼꺽꺾껌껍껏껑께껴꼬꼭꼴꼼꼽꼿꽁꽂꽃꽉꽝꽤꾀꾸꾹꾼꿀꿈꿍꿔꿨꿰뀌뀐뀝끄끈끊끌끓끔끗끙끝끼낄낌나낙낚난날낡남납낫났낭낮낯낱낳내낸낼냄냅냇냉냐냠냥너넉넌널넓넘넛넣네넥넨넬넷녀녁년념녕녘노녹논놀놈놉농높놓놔뇌뇨뇸누눈눌눔눕뉘뉴늄느늑는늘늙능늦늬니닉닌닐님닙닝다닥닦단닫달닭닮닳담답닷당닿대댁댄댐댓더덕던덜덟덤덥덧덩덫덮데덱덴델뎅뎌도독돈돌돔돕돗동돼됐되된될됨됩두둑둔둘둠둡둣둥뒤뒷듀드득든듣들듬듭듯등듸디딕딘딜딝딤딥딧딩딪따딱딴딸땀땅때땐땜땠땡떙떠떡떤떨떻떼또똑똘똥뚜뚝뚫뚱뛰뜀뜨뜩뜯뜰뜸뜻띄띠라락란랄람랍랐랑랗래랙랜램랫랬랭랲략량러럭런럴럼럽럿렀렁렇레렉렌렐렘렙렛려력련렬렴렵렷렸령례로록론롤롬롭롯롱뢰료룡루룩룬룰룸룹룻룽뤄뤼류륙륜률륭르륵른를름릅릇릉릎리릭린릴림립릿링마막만많말맑맘맙맛망맞맡맣매맥맨맵맹맺머먹먼멀멈멋멍멎메멘멜멤멩며면멸명몇모목몫몬몯몰몸몹못몽묘무묵묶문묻물뭄뭇뭉뭐뭔뭘뭣뮈뮌뮐뮤므믄믈미믹민믿밀밉밌밍및밑바박밖반받발밝밟밤밥방밭배백밴밸뱀뱃뱅뱉버벅번벌범법벗벙벚베벤벧벨벳벼벽변별볍병볕보복볶본볼봄봅봇봉봐봤뵈뵙부북분붇불붉붐붓붕붙뷔뷰브븐블븨비빅빈빌빔빗빙빚빛빠빡빨빱빵빼뺄뺏뺨뺴뻐뻔뻗뻬뼈뼉뼛뽀뽑뾰뿌뿍뿐뿔뿡쁘쁜쁨삐사삭산살삶삼삽삿샀상새색샌샐샘샛생샤샨샬샴샵샷샹섀서석섞선설섬섭섯성세섹센셀셈셉셋셔션셜셨셰셸소속손솔솜솝솟송솥쇄쇠쇼숍숏수숙순숟술숨숫숭숯숲숴쉐쉬쉰쉴쉼쉽쉿슈슐슛스슨슬슴습슷승시식신싣실싫심십싯싱싶싸싹싼쌀쌈쌌쌍쌓쌘쌤쌩써썩썬썰썹썼쎄쎈쎌쏘쏙쏟쑤쑥쓰쓴쓸씀씌씨씩씬씰씹씻씽아악안앉않알앓암압앗았앙앞애액앤앨앱앵야약얀얄얇양얕얗얘어억언얹얻얼엄업없엇었엉엊엌엎에엑엔엘엠엡엣엥여역엮연열엷염엽엿였영옆예옌옐옘옙옛오옥온올옮옳옵옷옹옻와왁완왓왔왕왜왠외왼요욕욘욜욤용우욱운울움웃웅워원월웠웨웩웬웰웹위윅윈윌윗윙유육윤율융윷으윽은을음읍응읗의이익인일읽잃임입잇있잉잊잎자작잔잖잘잠잡잣장잦재잭잰잼쟁쟤저적전절젊젋점접젓정젖제젝젠젤젯젱져졌조족존졸좀좁종좋좌죄죠주죽준줄줌줍중줘쥐쥘쥬쥴즈즉즌즐즘즙증지직진질짐집짓징짙짚짜짝짠짧짱째쨌쨰쩌쩍쩐쩔쩜쩰쪄쪽쫑쫓쬘쭈쭉쯔쯤찌찍찔찜찡찢찧차착찬찮찰참찻창찾채책챈챌챔챙챠처척천철첨첩첫청체첵첸첼쳐쳤초촉촌촘촛총촬최추축춘출춤춥춧충춰췌취츠측츰층치칙친칠침칩칫칭카칵칸칼캄캅캉캐캔캘캠캡캣캥커컥컨컬컴컵컷케켄켈켓켜켰코콕콘콜콤콥콧콩콰콸쾅쾌쿄쿠쿡쿤쿨쿵쿼퀀퀘퀴퀵퀸큐큘크큰클큼키킥킨킬킴킵킷킹타탁탄탈탉탐탑탓탕태택탠탤탬탭탱터턱턴털텀텃텅테텍텐텔템텝텨톈토톡톤톨톰톱톳통톺퇴투툭툰툴툼퉁튀튜트특튼튿틀틈티틱틴틸팀팁팃팅파팎판팔팜팝팟팡팥패팩팬팰팻팽퍼퍽펀펄펌페펙펜펠펩펫펭펴편펼평폐포폭폰폴폼퐁표푸푹푼풀품풋풍퓨퓰프픈플픔피픽핀필핌핏핑하학한할함합핫항핳해핵핸햄햇했행햐향허헌헐험헛헤헨헬헴헸혀혁현혈혐협혔형혜호혹혼홀홈홉홍화확환활황회획횐횟횡효후훈훌훔훤훨훼휀휘휙휠휴흄흉흐흑흔흘흙흠흡흥흩희흰히힌힐힘힝，．０１ＥＨＰＲＴａｋｌｍ'\n",
    "parser.image_folder = image_folder\n",
    "parser.saved_model = save_model_name\n",
    "\n",
    "# parser.character = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "opt = parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt.Transformation = 'TPS' \n",
    "# opt.FeatureExtraction = 'ResNet' \n",
    "# opt.SequenceModeling = 'BiLSTM'\n",
    "# opt.Prediction = 'Attn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.saved_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_list = demo(opt)\n",
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for image_tensors, image_path_list in demo_loader:\n",
    "\n",
    "        batch_size = image_tensors.size(0)\n",
    "        image = image_tensors.to(device)\n",
    "        print(image.shape)\n",
    "        # For max length prediction\n",
    "        length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "        text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "        print('데이터 텐서화')\n",
    "\n",
    "        if 'CTC' in opt.Prediction:\n",
    "            preds = model_c(image)\n",
    "            print(preds.shape)\n",
    "            # Select max probabilty (greedy decoding) then decode index to character\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "            _, preds_index = preds.max(2)\n",
    "            preds_index = preds_index.view(-1)\n",
    "            preds_str = converter.decode(preds_index.data, preds_size.data)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print('182')\n",
    "            preds = model_c(image)\n",
    "\n",
    "            # select max probabilty (greedy decoding) then decode index to character\n",
    "            _, preds_index = preds.max(2)\n",
    "            preds_str = converter.decode(preds_index, length_for_pred)\n",
    "#             return model, image, text_for_pred, preds, preds_index, preds_str\n",
    "\n",
    "        preds_prob = F.softmax(preds, dim=2)\n",
    "        preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "        count = 0\n",
    "\n",
    "        for img_name, pred, pred_max_prob in zip(image_path_list, preds_str, preds_max_prob):\n",
    "            count +=1\n",
    "            if 'Attn' in opt.Prediction:\n",
    "                pred_EOS = pred.find('[s]')\n",
    "                pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "                pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "            # calculate confidence score (= multiply of pred_max_prob)\n",
    "            confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "            result_list.append([img_name,pred,confidence_score])\n",
    "            print('183')\n",
    "            print(f'{img_name:25s}\\t{pred:25s}\\t{confidence_score:0.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### model 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\"\"\" model configuration \"\"\"\n",
    "if 'CTC' in opt.Prediction:\n",
    "    converter = CTCLabelConverter(opt.character)\n",
    "    print('ctc')\n",
    "else:\n",
    "    converter = AttnLabelConverter(opt.character)\n",
    "opt.num_class = len(converter.character)\n",
    "print(opt.num_class)\n",
    "if opt.rgb:\n",
    "    opt.input_channel = 3\n",
    "model = Model(opt)\n",
    "\n",
    "model.load_state_dict(copyStateDict(torch.load(opt.saved_model, map_location=device)))\n",
    "\n",
    "model = model.to(device)\n",
    "print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "      opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "      opt.SequenceModeling, opt.Prediction)\n",
    "# model = torch.nn.DataParallel(model).to(device)\n",
    "\n",
    "# load model\n",
    "print('loading pretrained model from %s' % opt.saved_model)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt.Transformation = 'None' \n",
    "# opt.FeatureExtraction = 'ResNet' \n",
    "# opt.SequenceModeling = 'BiLSTM' \n",
    "opt.Prediction = 'CTC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c = Model(opt)\n",
    "\n",
    "model_c.FeatureExtraction = model.FeatureExtraction\n",
    "\n",
    "model_c.AdaptiveAvgPool = model.AdaptiveAvgPool\n",
    "model_c.SequenceModeling = model.SequenceModeling\n",
    "model_c.Prediction = model.Prediction.generator\n",
    "model_c = model_c.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare data. two demo images from https://github.com/bgshih/crnn#run-demo\n",
    "AlignCollate_demo = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
    "demo_data = RawDataset(root=opt.image_folder, opt=opt)  # use RawDataset\n",
    "demo_loader = torch.utils.data.DataLoader(\n",
    "    demo_data, batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=int(opt.workers),\n",
    "    collate_fn=AlignCollate_demo, pin_memory=True)\n",
    "print('데이터 생성완료')\n",
    "# predict\n",
    "result_list = []\n",
    "with torch.no_grad():\n",
    "\n",
    "    for image_tensors, image_path_list in demo_loader:\n",
    "\n",
    "        batch_size = image_tensors.size(0)\n",
    "        image = image_tensors.to(device)\n",
    "        # For max length prediction\n",
    "        length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "#         text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "#         print('text_for_pred',text_for_pred.shape)\n",
    "        print('데이터 텐서화')\n",
    "#         predic = model_c(image)\n",
    "#         print(preds.shape)\n",
    "        dynamic_axes = {\"input\": {0: \"batch_size\"}, \n",
    "                        \"output\": {0: \"batch_size\"}\n",
    "                                       }\n",
    "        output_path = './weights'\n",
    "        # Set name for onnx files\n",
    "        output_detec = os.path.join(output_path, \"ocr.onnx\")\n",
    "        \n",
    "        torch.onnx.export(model_c, image, output_detec,\n",
    "                           verbose=True, \n",
    "                           opset_version=13,\n",
    "                           do_constant_folding=True,\n",
    "                           export_params=True,\n",
    "                           input_names=[\"input\"],\n",
    "                           output_names=[\"output\"],\n",
    "                           dynamic_axes=dynamic_axes\n",
    "                                                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = True\n",
    "opt.num_gpu = torch.cuda.device_count()\n",
    "result_list= demo(opt)\n",
    "\n",
    "# model, image, text_for_pred = demo(opt)\n",
    "\n",
    "t1 = time.time()\n",
    "print(t1-t0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_list = model, image, text_for_pred, preds, preds_index, preds_str\n",
    "model = result_list[0]\n",
    "image = result_list[1]\n",
    "text_for_pred = result_list[2]\n",
    "preds = result_list[3]\n",
    "preds_index = result_list[4]\n",
    "preds_str = result_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(image, text_for_pred, is_train=False)\n",
    "_, preds_index = preds.max(2)\n",
    "preds_str = converter.decode(preds_index, length_for_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ocr_df = pd.DataFrame(result_list,columns=['path','ocr','score'])\n",
    "ocr_df['score'] = ocr_df.score.astype(float)\n",
    "ocr_df['ocr'] = ocr_df.ocr.apply(lambda x: x.replace(' ',''))\n",
    "ocr_df['score'] = ocr_df.score.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_file_path= \"./weights/ocr.onnx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "\n",
    "# Load the ONNX model\n",
    "model = onnx.load(onnx_file_path)\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "from skimage import io\n",
    "import cv2\n",
    "\n",
    "def loadImage(img_file):\n",
    "    img = io.imread(img_file)           # RGB order\n",
    "    if img.shape[0] == 2: img = img[0]\n",
    "    if len(img.shape) == 2 : img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    if img.shape[2] == 4:   img = img[:,:,:3]\n",
    "    img = np.array(img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "# image_path = './IMG_8178.jpg'\n",
    "# print(\"Test image :\", image_path)\n",
    "\n",
    "# image = imgproc.loadImage(image_path)\n",
    "\n",
    "ort_session = ort.InferenceSession(onnx_file_path)\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# ONNX 런타임에서 계산된 결과값\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(image)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = torch.from_numpy(ort_outs[0])\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def pred_result(preds):\n",
    "    batch_size = preds.size(0)\n",
    "    preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "    _, preds_index = preds.max(2)\n",
    "    preds_index = preds_index.view(-1)\n",
    "\n",
    "    preds_str = []\n",
    "    for index, l in enumerate(preds_size.data):\n",
    "\n",
    "        t = preds_index.data[index:]\n",
    "\n",
    "        char_list = []\n",
    "        for i in range(l):\n",
    "            if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank.\n",
    "                char_list.append(opt.character[t[i]])\n",
    "        text = ''.join(char_list)\n",
    "\n",
    "        preds_str.append(text)\n",
    "\n",
    "\n",
    "    preds_prob = F.softmax(preds, dim=2)\n",
    "    preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "\n",
    "    result_list = []\n",
    "    for pred, pred_max_prob in zip(preds_str, preds_max_prob):\n",
    "        if 'Attn' in opt.Prediction:\n",
    "            pred_EOS = pred.find('[s]')\n",
    "            pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "            pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "        # calculate confidence score (= multiply of pred_max_prob)\n",
    "        try:\n",
    "            confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "        except:\n",
    "            confidence_score  = 0\n",
    "        result_list.append([pred, float(confidence_score)])\n",
    "        \n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = pred_result(predic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
