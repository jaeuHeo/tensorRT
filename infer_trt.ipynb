{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bfd173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "# os.environ['NVIDIA_VISIBLE_DEVICES'] = \"0\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212aa5a3",
   "metadata": {},
   "source": [
    "# detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ba0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self,cuda=True, trained_model='weights/craft_mlt_25k.pth', text_threshold=0.7, low_text=0.4, link_threshold=0.4, canvas_size =1100, mag_ratio=1.5, poly=False, show_time=False,test_folder='/data/',refine=False, refiner_model='weights/craft_refiner_CTW1500.pth'):\n",
    "        self.cuda = cuda\n",
    "        self.trained_model = trained_model = trained_model\n",
    "        self.text_threshold = text_threshold\n",
    "        self.low_text = low_text\n",
    "        self.link_threshold = link_threshold\n",
    "        self.canvas_size = canvas_size\n",
    "        self.mag_ratio = mag_ratio\n",
    "        self.poly = poly\n",
    "        self.show_time = show_time\n",
    "        self.test_folder = test_folder\n",
    "        self.refine = refine\n",
    "        self.refiner_model = refiner_model\n",
    "        \n",
    "def img_show(img, size =(15,15)):\n",
    "    plt.rcParams[\"figure.figsize\"] = size\n",
    "    imgplot = plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e76321",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bfd9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import cv2\n",
    "import math\n",
    "\n",
    "\"\"\" auxilary functions \"\"\"\n",
    "# unwarp corodinates\n",
    "def warpCoord(Minv, pt):\n",
    "    out = np.matmul(Minv, (pt[0], pt[1], 1))\n",
    "    return np.array([out[0]/out[2], out[1]/out[2]])\n",
    "\"\"\" end of auxilary functions \"\"\"\n",
    "\n",
    "\n",
    "def getDetBoxes_core(textmap, linkmap, text_threshold, link_threshold, low_text):\n",
    "    # prepare data\n",
    "    linkmap = linkmap.copy()\n",
    "    textmap = textmap.copy()\n",
    "    img_h, img_w = textmap.shape\n",
    "    \"\"\" labeling method \"\"\"\n",
    "    ret, text_score = cv2.threshold(textmap, low_text, 1, 0)\n",
    "    ret, link_score = cv2.threshold(linkmap, link_threshold, 1, 0)\n",
    "\n",
    "    text_score_comb = np.clip(text_score + link_score, 0, 1)\n",
    "    nLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(text_score_comb.astype(np.uint8), connectivity=4)\n",
    "\n",
    "    det = []\n",
    "    mapper = []\n",
    "    for k in range(1,nLabels):\n",
    "        # size filtering\n",
    "        size = stats[k, cv2.CC_STAT_AREA]\n",
    "        if size < 10: continue\n",
    "\n",
    "        # thresholding\n",
    "        if np.max(textmap[labels==k]) < text_threshold: continue\n",
    "\n",
    "        # make segmentation map\n",
    "        segmap = np.zeros(textmap.shape, dtype=np.uint8)\n",
    "        segmap[labels==k] = 255\n",
    "        segmap[np.logical_and(link_score==1, text_score==0)] = 0   # remove link area\n",
    "        x, y = stats[k, cv2.CC_STAT_LEFT], stats[k, cv2.CC_STAT_TOP]\n",
    "        w, h = stats[k, cv2.CC_STAT_WIDTH], stats[k, cv2.CC_STAT_HEIGHT]\n",
    "        niter = int(math.sqrt(size * min(w, h) / (w * h)) * 2)\n",
    "        sx, ex, sy, ey = x - niter, x + w + niter + 1, y - niter, y + h + niter + 1\n",
    "        # boundary check\n",
    "        if sx < 0 : sx = 0\n",
    "        if sy < 0 : sy = 0\n",
    "        if ex >= img_w: ex = img_w\n",
    "        if ey >= img_h: ey = img_h\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT,(1 + niter, 1 + niter))\n",
    "        segmap[sy:ey, sx:ex] = cv2.dilate(segmap[sy:ey, sx:ex], kernel)\n",
    "\n",
    "        # make box\n",
    "        np_contours = np.roll(np.array(np.where(segmap!=0)),1,axis=0).transpose().reshape(-1,2)\n",
    "        rectangle = cv2.minAreaRect(np_contours)\n",
    "        box = cv2.boxPoints(rectangle)\n",
    "\n",
    "        # align diamond-shape\n",
    "        w, h = np.linalg.norm(box[0] - box[1]), np.linalg.norm(box[1] - box[2])\n",
    "        box_ratio = max(w, h) / (min(w, h) + 1e-5)\n",
    "        if abs(1 - box_ratio) <= 0.1:\n",
    "            l, r = min(np_contours[:,0]), max(np_contours[:,0])\n",
    "            t, b = min(np_contours[:,1]), max(np_contours[:,1])\n",
    "            box = np.array([[l, t], [r, t], [r, b], [l, b]], dtype=np.float32)\n",
    "\n",
    "        # make clock-wise order\n",
    "        startidx = box.sum(axis=1).argmin()\n",
    "        box = np.roll(box, 4-startidx, 0)\n",
    "        box = np.array(box)\n",
    "\n",
    "        det.append(box)\n",
    "        mapper.append(k)\n",
    "\n",
    "    return det, labels, mapper\n",
    "\n",
    "def getPoly_core(boxes, labels, mapper, linkmap):\n",
    "    # configs\n",
    "    num_cp = 5\n",
    "    max_len_ratio = 0.7\n",
    "    expand_ratio = 1.45\n",
    "    max_r = 2.0\n",
    "    step_r = 0.2\n",
    "\n",
    "    polys = []  \n",
    "    for k, box in enumerate(boxes):\n",
    "        # size filter for small instance\n",
    "        w, h = int(np.linalg.norm(box[0] - box[1]) + 1), int(np.linalg.norm(box[1] - box[2]) + 1)\n",
    "        if w < 10 or h < 10:\n",
    "            polys.append(None); continue\n",
    "\n",
    "        # warp image\n",
    "        tar = np.float32([[0,0],[w,0],[w,h],[0,h]])\n",
    "        M = cv2.getPerspectiveTransform(box, tar)\n",
    "        word_label = cv2.warpPerspective(labels, M, (w, h), flags=cv2.INTER_NEAREST)\n",
    "        try:\n",
    "            Minv = np.linalg.inv(M)\n",
    "        except:\n",
    "            polys.append(None); continue\n",
    "\n",
    "        # binarization for selected label\n",
    "        cur_label = mapper[k]\n",
    "        word_label[word_label != cur_label] = 0\n",
    "        word_label[word_label > 0] = 1\n",
    "\n",
    "        \"\"\" Polygon generation \"\"\"\n",
    "        # find top/bottom contours\n",
    "        cp = []\n",
    "        max_len = -1\n",
    "        for i in range(w):\n",
    "            region = np.where(word_label[:,i] != 0)[0]\n",
    "            if len(region) < 2 : continue\n",
    "            cp.append((i, region[0], region[-1]))\n",
    "            length = region[-1] - region[0] + 1\n",
    "            if length > max_len: max_len = length\n",
    "\n",
    "        # pass if max_len is similar to h\n",
    "        if h * max_len_ratio < max_len:\n",
    "            polys.append(None); continue\n",
    "\n",
    "        # get pivot points with fixed length\n",
    "        tot_seg = num_cp * 2 + 1\n",
    "        seg_w = w / tot_seg     # segment width\n",
    "        pp = [None] * num_cp    # init pivot points\n",
    "        cp_section = [[0, 0]] * tot_seg\n",
    "        seg_height = [0] * num_cp\n",
    "        seg_num = 0\n",
    "        num_sec = 0\n",
    "        prev_h = -1\n",
    "        for i in range(0,len(cp)):\n",
    "            (x, sy, ey) = cp[i]\n",
    "            if (seg_num + 1) * seg_w <= x and seg_num <= tot_seg:\n",
    "                # average previous segment\n",
    "                if num_sec == 0: break\n",
    "                cp_section[seg_num] = [cp_section[seg_num][0] / num_sec, cp_section[seg_num][1] / num_sec]\n",
    "                num_sec = 0\n",
    "\n",
    "                # reset variables\n",
    "                seg_num += 1\n",
    "                prev_h = -1\n",
    "\n",
    "            # accumulate center points\n",
    "            cy = (sy + ey) * 0.5\n",
    "            cur_h = ey - sy + 1\n",
    "            cp_section[seg_num] = [cp_section[seg_num][0] + x, cp_section[seg_num][1] + cy]\n",
    "            num_sec += 1\n",
    "\n",
    "            if seg_num % 2 == 0: continue # No polygon area\n",
    "\n",
    "            if prev_h < cur_h:\n",
    "                pp[int((seg_num - 1)/2)] = (x, cy)\n",
    "                seg_height[int((seg_num - 1)/2)] = cur_h\n",
    "                prev_h = cur_h\n",
    "\n",
    "        # processing last segment\n",
    "        if num_sec != 0:\n",
    "            cp_section[-1] = [cp_section[-1][0] / num_sec, cp_section[-1][1] / num_sec]\n",
    "\n",
    "        # pass if num of pivots is not sufficient or segment widh is smaller than character height \n",
    "        if None in pp or seg_w < np.max(seg_height) * 0.25:\n",
    "            polys.append(None); continue\n",
    "\n",
    "        # calc median maximum of pivot points\n",
    "        half_char_h = np.median(seg_height) * expand_ratio / 2\n",
    "\n",
    "        # calc gradiant and apply to make horizontal pivots\n",
    "        new_pp = []\n",
    "        for i, (x, cy) in enumerate(pp):\n",
    "            dx = cp_section[i * 2 + 2][0] - cp_section[i * 2][0]\n",
    "            dy = cp_section[i * 2 + 2][1] - cp_section[i * 2][1]\n",
    "            if dx == 0:     # gradient if zero\n",
    "                new_pp.append([x, cy - half_char_h, x, cy + half_char_h])\n",
    "                continue\n",
    "            rad = - math.atan2(dy, dx)\n",
    "            c, s = half_char_h * math.cos(rad), half_char_h * math.sin(rad)\n",
    "            new_pp.append([x - s, cy - c, x + s, cy + c])\n",
    "\n",
    "        # get edge points to cover character heatmaps\n",
    "        isSppFound, isEppFound = False, False\n",
    "        grad_s = (pp[1][1] - pp[0][1]) / (pp[1][0] - pp[0][0]) + (pp[2][1] - pp[1][1]) / (pp[2][0] - pp[1][0])\n",
    "        grad_e = (pp[-2][1] - pp[-1][1]) / (pp[-2][0] - pp[-1][0]) + (pp[-3][1] - pp[-2][1]) / (pp[-3][0] - pp[-2][0])\n",
    "        for r in np.arange(0.5, max_r, step_r):\n",
    "            dx = 2 * half_char_h * r\n",
    "            if not isSppFound:\n",
    "                line_img = np.zeros(word_label.shape, dtype=np.uint8)\n",
    "                dy = grad_s * dx\n",
    "                p = np.array(new_pp[0]) - np.array([dx, dy, dx, dy])\n",
    "                cv2.line(line_img, (int(p[0]), int(p[1])), (int(p[2]), int(p[3])), 1, thickness=1)\n",
    "                if np.sum(np.logical_and(word_label, line_img)) == 0 or r + 2 * step_r >= max_r:\n",
    "                    spp = p\n",
    "                    isSppFound = True\n",
    "            if not isEppFound:\n",
    "                line_img = np.zeros(word_label.shape, dtype=np.uint8)\n",
    "                dy = grad_e * dx\n",
    "                p = np.array(new_pp[-1]) + np.array([dx, dy, dx, dy])\n",
    "                cv2.line(line_img, (int(p[0]), int(p[1])), (int(p[2]), int(p[3])), 1, thickness=1)\n",
    "                if np.sum(np.logical_and(word_label, line_img)) == 0 or r + 2 * step_r >= max_r:\n",
    "                    epp = p\n",
    "                    isEppFound = True\n",
    "            if isSppFound and isEppFound:\n",
    "                break\n",
    "\n",
    "        # pass if boundary of polygon is not found\n",
    "        if not (isSppFound and isEppFound):\n",
    "            polys.append(None); continue\n",
    "\n",
    "        # make final polygon\n",
    "        poly = []\n",
    "        poly.append(warpCoord(Minv, (spp[0], spp[1])))\n",
    "        for p in new_pp:\n",
    "            poly.append(warpCoord(Minv, (p[0], p[1])))\n",
    "        poly.append(warpCoord(Minv, (epp[0], epp[1])))\n",
    "        poly.append(warpCoord(Minv, (epp[2], epp[3])))\n",
    "        for p in reversed(new_pp):\n",
    "            poly.append(warpCoord(Minv, (p[2], p[3])))\n",
    "        poly.append(warpCoord(Minv, (spp[2], spp[3])))\n",
    "\n",
    "        # add to final result\n",
    "        polys.append(np.array(poly))\n",
    "\n",
    "    return polys\n",
    "\n",
    "def getDetBoxes(textmap, linkmap, text_threshold, link_threshold, low_text, poly=False):\n",
    "    \n",
    "    boxes, labels, mapper = getDetBoxes_core(textmap, linkmap, text_threshold, link_threshold, low_text)\n",
    "\n",
    "    if poly:\n",
    "        polys = getPoly_core(boxes, labels, mapper, linkmap)\n",
    "    else:\n",
    "        polys = [None] * len(boxes)\n",
    "\n",
    "    return boxes, polys\n",
    "\n",
    "def adjustResultCoordinates(polys, ratio_w, ratio_h, ratio_net = 2):\n",
    "    if len(polys) > 0:\n",
    "        polys = np.array(polys)\n",
    "        for k in range(len(polys)):\n",
    "            if polys[k] is not None:\n",
    "                polys[k] *= (ratio_w * ratio_net, ratio_h * ratio_net)\n",
    "    return polys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aff0122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "# import craft_utils\n",
    "import imgproc\n",
    "import file_utils\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import tensorrt as trt\n",
    "\n",
    "from icecream import ic\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "TRT_LOGGER = trt.Logger()\n",
    "\n",
    "class RTLayer():\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config=None, model_path=None, data_path='./weights',\n",
    "                 engine_path=None, cuda_ctx=None, input_shape=None):\n",
    "        super().__init__()\n",
    "        data_path = os.path.abspath(data_path)\n",
    "        model_path = sorted(glob(data_path + '/*.engine'))\n",
    "    \n",
    "#         self.engine_path=model_path[0]\n",
    "        self.engine_path=data_path+'/detect.engine'\n",
    "        \n",
    "        self.cuda_ctx = cuda_ctx\n",
    "        if self.cuda_ctx:\n",
    "            self.cuda_ctx.push()\n",
    "\n",
    "        self.trt_logger = trt.Logger(trt.Logger.INFO)\n",
    "        self._load_plugins()\n",
    "        self.engine = self._load_engine()\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def __call__(self, args, image, arg_cuda):\n",
    "        \n",
    "        trt_infer_befo_mem = torch.cuda.memory_allocated()/1024/1024\n",
    "        print(\"current_memory:\", trt_infer_befo_mem)\n",
    "        \n",
    "\n",
    "        # resize\n",
    "        img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(image, args.canvas_size, interpolation=cv2.INTER_LINEAR, mag_ratio=args.mag_ratio)\n",
    "        ratio_h = ratio_w = 1 / target_ratio\n",
    "        \n",
    "#         t0 = time.time()\n",
    "        # preprocessing\n",
    "        img_resized = imgproc.normalizeMeanVariance(img_resized)\n",
    "        img_resized = torch.from_numpy(img_resized).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n",
    "        img_resized = Variable(img_resized.unsqueeze(0))                # [c, h, w] to [b, c, h, w]\n",
    "#         t0 = time.time() - t0\n",
    "#         print(t0)\n",
    "        t0 = time.time()\n",
    "#         if arg_cuda:\n",
    "#             img_resized.cuda()\n",
    "#             img_resized = img_resized.to(device)\n",
    "        t0 = time.time() - t0\n",
    "        print(t0)\n",
    "        # ic(img_resized.shape)\n",
    "        \n",
    "        # feed to engine and process output\n",
    "        height, width = img_resized.shape[2:4]\n",
    "        self.input_shape = (height,width)\n",
    "        img_resized = img_resized.cpu().detach().numpy()\n",
    "#         print('img2', img_resized.shape, img_resized)\n",
    "        \n",
    "        segment_inputs, segment_outputs, segment_bindings = self._allocate_buffers()\n",
    "        \n",
    "        stream = cuda.Stream()\n",
    "        with self.engine.create_execution_context() as context:\n",
    "            context.active_optimization_profile = 0\n",
    "            origin_inputshape=context.get_binding_shape(0)\n",
    "            \n",
    "            if (origin_inputshape[-1]==-1):\n",
    "                origin_inputshape[-2],origin_inputshape[-1]=(self.input_shape)\n",
    "                context.set_binding_shape(0,(origin_inputshape))\n",
    "            \n",
    "            input_img_array = np.array([img_resized] * self.engine.max_batch_size)\n",
    "            \n",
    "            img = torch.from_numpy(input_img_array).float().numpy()\n",
    "\n",
    "            segment_inputs[0].host = img\n",
    "\n",
    "            [cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in segment_inputs] #Copy from the Python buffer src to the device pointer dest (an int or a DeviceAllocation) asynchronously,\n",
    "#             segment_inputs[0].device\n",
    "            stream.synchronize() #Wait for all activity on this stream to cease, then return.\n",
    "            \n",
    "            context.execute_async_v2(bindings=segment_bindings, stream_handle=stream.handle)#Asynchronously execute inference on a batch. \n",
    "            \n",
    "            stream.synchronize()\n",
    "            [cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in segment_outputs]#Copy from the device pointer src (an int or a DeviceAllocation) to the Python buffer dest asynchronously\n",
    "            stream.synchronize()\n",
    "            bs1, bs2 = context.get_binding_shape(2),context.get_binding_shape(1)\n",
    "\n",
    "            y_out = segment_outputs[1].host\n",
    "            feature_out = segment_outputs[0].host\n",
    "\n",
    "        t1 = time.time()\n",
    "        y1 =  y_out[0:np.array(bs1).prod()].reshape(bs1)\n",
    "        feature1 = feature_out[0:np.array(bs2).prod()].reshape(bs2)\n",
    "        \n",
    "        y = torch.from_numpy(y1)\n",
    "        feature = torch.from_numpy(feature1)\n",
    "        \n",
    "        # make score and link map\n",
    "        score_text = y[0,:,:,0].cpu().data.numpy()\n",
    "        score_link = y[0,:,:,1].cpu().data.numpy()\n",
    "        \n",
    "        # refine link\n",
    "        if refine_net is not None:\n",
    "            with torch.no_grad():\n",
    "                y_refiner = refine_net(y, feature)\n",
    "            score_link = y_refiner[0,:,:,0].cpu().data.numpy()\n",
    "\n",
    "        t1 = time.time() - t1\n",
    "        print(t1)\n",
    "        # Post-processing\n",
    "        boxes, polys = getDetBoxes(score_text, score_link, args.text_threshold, args.link_threshold, args.low_text, args.poly)\n",
    "\n",
    "        # coordinate adjustment\n",
    "        boxes = adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
    "        polys = adjustResultCoordinates(polys, ratio_w, ratio_h)\n",
    "        for k in range(len(polys)):\n",
    "            if polys[k] is None: polys[k] = boxes[k]\n",
    "\n",
    "        # render results (optional)\n",
    "        render_img = score_text.copy()\n",
    "        render_img = np.hstack((render_img, score_link))\n",
    "        ret_score_text = imgproc.cvt2HeatmapImg(render_img)\n",
    "        \n",
    "#         print(\"\\ninfer/postproc time : {:.3f}/{:.3f}\".format(t0, t1))\n",
    "        \n",
    "        trt_infer_mem = torch.cuda.memory_allocated()/1024/1024\n",
    "        print(\"trt infer memory: %fMB\"%(trt_infer_mem-trt_infer_befo_mem))\n",
    "        \n",
    "        return boxes, polys, ret_score_text\n",
    "        \n",
    "    def _load_plugins(self):\n",
    "        if trt.__version__[0] < '7':\n",
    "            ctypes.CDLL(\"./libflattenconcat.so\")\n",
    "        trt.init_libnvinfer_plugins(self.trt_logger, '')\n",
    "        print('success load pluginx')\n",
    "        \n",
    "    def _load_engine(self):\n",
    "        assert os.path.exists(self.engine_path)\n",
    "        print(\"Reading engine from file {}\".format(self.engine_path))\n",
    "        with open(self.engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "            print('success load engine')\n",
    "            return runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "    def _allocate_buffers(self):\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        bindings = []\n",
    "        class HostDeviceMem(object):\n",
    "            def __init__(self, host_mem, device_mem):\n",
    "                self.host = host_mem\n",
    "                self.device = device_mem\n",
    "\n",
    "            def __str__(self):\n",
    "                return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "            \n",
    "            def __repr__(self):\n",
    "                return self.__str__()\n",
    "        \n",
    "        for binding in self.engine:\n",
    "            \n",
    "            dims = self.engine.get_binding_shape(binding)\n",
    "            # print(dims)\n",
    "            if dims[-1] == -1:\n",
    "                assert(self.input_shape is not None)\n",
    "                dims[-2],dims[-1] = self.input_shape\n",
    "            else:\n",
    "                dims[-3],dims[-2] = int(self.input_shape[0]/2), int(self.input_shape[1]/2)\n",
    "            size = trt.volume(dims) * self.engine.max_batch_size#The maximum batch size which can be used for inference.\n",
    "            dtype = trt.nptype(self.engine.get_binding_dtype(binding))\n",
    "            # Allocate host and device buffers\n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            # Append the device buffer to device bindings.\n",
    "            bindings.append(int(device_mem))\n",
    "            if self.engine.binding_is_input(binding):#Determine whether a binding is an input binding.\n",
    "                inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "            else:\n",
    "                outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        \n",
    "        print('success allocate_buffers')\n",
    "        return inputs, outputs, bindings\n",
    "\n",
    "    # def __del__(self):\n",
    "    #     \"\"\"Free CUDA memories and context.\"\"\"\n",
    "    #     del self.cuda_outputs\n",
    "    #     del self.cuda_inputs\n",
    "    #     del self.stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016c94fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgproc\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "image_path = './IMG_7602.jpg'\n",
    "print(\"Test image :\", image_path)\n",
    "\n",
    "image = imgproc.loadImage(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6a0e19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "_____________________________________________________________________________\n",
    "This file contains main inference pipeline to Tensor RT\n",
    "_____________________________________________________________________________\n",
    "\"\"\"\n",
    "from icecream import ic\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "from skimage import io\n",
    "import imgproc\n",
    "import file_utils\n",
    "\n",
    "# from trt_layer import RTLayer\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"y\", \"true\", \"t\", \"1\")\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='TensorRT inference pipeline for CRAFT Text Detection')\n",
    "# parser.add_argument('--text_threshold', default=0.7, type=float, help='text confidence threshold')\n",
    "# parser.add_argument('--low_text', default=0.4, type=float, help='text low-bound score')\n",
    "# parser.add_argument('--link_threshold', default=0.4, type=float, help='link confidence threshold')\n",
    "# parser.add_argument('--canvas_size', default=1100, type=int, help='image size for inference')\n",
    "# parser.add_argument('--mag_ratio', default=1.5, type=float, help='image magnification ratio')\n",
    "# parser.add_argument('--poly', default=False, action='store_true', help='enable polygon type')\n",
    "# parser.add_argument('--show_time', default=False, action='store_true', help='show processing time')\n",
    "# parser.add_argument('--test_folder', default='images/', type=str, help='folder path to input images')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "\n",
    "\"\"\" For test images in a folder \"\"\"\n",
    "\n",
    "result_folder = './result/'\n",
    "if not os.path.isdir(result_folder):\n",
    "    os.mkdir(result_folder)\n",
    "\n",
    "def test_net(args, image):\n",
    "    layer = RTLayer()\n",
    "    t = time.time()\n",
    "    boxes, polys, ret_score_text = layer(args, image)\n",
    "    print(f'infer time: {time.time()-t}')\n",
    "    return boxes, polys, ret_score_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb0a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinkRefiner\n",
    "refine_net = None\n",
    "if args.refine:\n",
    "    from refinenet import RefineNet\n",
    "    refine_net = RefineNet()\n",
    "    print('Loading weights of refiner from checkpoint (' + args.refiner_model + ')')\n",
    "    if args.cuda:\n",
    "        refine_net.load_state_dict(copyStateDict(torch.load(args.refiner_model)))\n",
    "        refine_net = refine_net.cuda()\n",
    "        refine_net = torch.nn.DataParallel(refine_net)\n",
    "    else:\n",
    "        refine_net.load_state_dict(copyStateDict(torch.load(args.refiner_model, map_location='cpu')))\n",
    "\n",
    "    refine_net.eval()\n",
    "    args.poly = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ae3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_load_befo_mem = torch.cuda.memory_allocated()/1024/1024\n",
    "print(\"current_memory:\", trt_load_befo_mem)\n",
    "t2 = time.time()\n",
    "layer = RTLayer()\n",
    "print(f'load time: {time.time()-t2}')\n",
    "\n",
    "trt_load_mem = torch.cuda.memory_allocated()/1024/1024\n",
    "\n",
    "print(\"trt_load_mem: %fMB\"%(trt_load_mem-trt_load_befo_mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157fd642",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "boxes, polys, ret_score_text = layer(args, image, args.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c0724c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for idx,i in enumerate(image_boxes):\n",
    "img_1 = image.copy()\n",
    "imageWidth, imageHeight = img_1.shape[:2] \n",
    "resizeHeight = int(1 * imageHeight) \n",
    "resizeWidth = int(1 * imageWidth) \n",
    "resizeImageNDArray = cv2.resize(img_1, (resizeHeight, resizeWidth), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "for pts in boxes:\n",
    "\n",
    "    pts = np.array(pts).reshape(-1,2).astype(np.int32)\n",
    "\n",
    "    resizeImageNDArray = cv2.polylines(resizeImageNDArray, [pts], True, (0,0,255),2)\n",
    "\n",
    "img_show(resizeImageNDArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee927d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from text_detection import craft_utils\n",
    "from text_detection import imgproc\n",
    "import text_detection.file_utils\n",
    "import json\n",
    "import zipfile\n",
    "\n",
    "from text_detection.craft import CRAFT\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def copyStateDict(state_dict):\n",
    "    if list(state_dict.keys())[0].startswith(\"module\"):\n",
    "        start_idx = 1\n",
    "    else:\n",
    "        start_idx = 0\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = \".\".join(k.split(\".\")[start_idx:])\n",
    "        new_state_dict[name] = v\n",
    "    return new_state_dict\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"y\", \"true\", \"t\", \"1\")\n",
    "\n",
    "def test_net(net, image, text_threshold, link_threshold, low_text, cuda, poly,refine_net=None):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # resize\n",
    "    img_resized, target_ratio, size_heatmap = imgproc.resize_aspect_ratio(image, args.canvas_size, interpolation=cv2.INTER_LINEAR, mag_ratio=args.mag_ratio)\n",
    "    ratio_h = ratio_w = 1 / target_ratio\n",
    "\n",
    "    # preprocessing\n",
    "    x = imgproc.normalizeMeanVariance(img_resized)\n",
    "    \n",
    "    x = torch.from_numpy(x).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n",
    "    x = Variable(x.unsqueeze(0))                # [c, h, w] to [b, c, h, w]\n",
    "    if cuda:\n",
    "        x = x.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        y, feature = net(x)\n",
    "    \n",
    "    # make score and link map\n",
    "    score_text = y[0,:,:,0].cpu().data.numpy()\n",
    "    score_link = y[0,:,:,1].cpu().data.numpy()\n",
    "\n",
    "    # refine link\n",
    "    if refine_net is not None:\n",
    "        with torch.no_grad():\n",
    "            y_refiner = refine_net(y, feature)\n",
    "        score_link = y_refiner[0,:,:,0].cpu().data.numpy()\n",
    "\n",
    "\n",
    "    # Post-processing\n",
    "    boxes, polys = craft_utils.getDetBoxes(score_text, score_link, text_threshold, link_threshold, low_text, poly)\n",
    "\n",
    "    # coordinate adjustment\n",
    "    boxes = craft_utils.adjustResultCoordinates(boxes, ratio_w, ratio_h)\n",
    "    polys = craft_utils.adjustResultCoordinates(polys, ratio_w, ratio_h)\n",
    "    for k in range(len(polys)):\n",
    "        if polys[k] is None: polys[k] = boxes[k]\n",
    "\n",
    "    \n",
    "    # render results (optional)\n",
    "    render_img = score_text.copy()\n",
    "    render_img = np.hstack((render_img, score_link))\n",
    "    ret_score_text = imgproc.cvt2HeatmapImg(render_img)\n",
    "    \n",
    "    if args.show_time : print(\"\\ninfer/postproc time : {:.3f}/{:.3f}\".format(t0, t1))\n",
    "\n",
    "    return boxes, polys, ret_score_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f9283",
   "metadata": {},
   "outputs": [],
   "source": [
    "regist_img_num = 0\n",
    "# test_img_num = 1\n",
    "\n",
    "# load net\n",
    "net = CRAFT()     # initialize\n",
    "\n",
    "print('Loading weights from checkpoint (' + args.trained_model + ')')\n",
    "device=torch.device('cuda')\n",
    "if args.cuda:\n",
    "    net.load_state_dict(copyStateDict(torch.load(args.trained_model,map_location=device)))\n",
    "else:\n",
    "    net.load_state_dict(copyStateDict(torch.load(args.trained_model, map_location='cpu')))\n",
    "\n",
    "if args.cuda:\n",
    "#     net = net.cuda()\n",
    "    net = net.to(device)\n",
    "    net = torch.nn.DataParallel(net)\n",
    "    cudnn.benchmark = False\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6480614",
   "metadata": {},
   "outputs": [],
   "source": [
    "bboxes, polys, score_text = test_net(net, image, args.text_threshold, args.link_threshold, args.low_text, args.cuda, args.poly, refine_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39453663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx,i in enumerate(image_boxes):\n",
    "img_2 = image.copy()\n",
    "imageWidth, imageHeight = img_2.shape[:2] \n",
    "resizeHeight = int(1 * imageHeight) \n",
    "resizeWidth = int(1 * imageWidth) \n",
    "resizeImageNDArray = cv2.resize(img_2, (resizeHeight, resizeWidth), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "for pts in bboxes:\n",
    "\n",
    "    pts = np.array(pts).reshape(-1,2).astype(np.int32)\n",
    "\n",
    "    resizeImageNDArray = cv2.polylines(resizeImageNDArray, [pts], True, (0,0,255),2)\n",
    "\n",
    "img_show(resizeImageNDArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c978bfb6",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29d2833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_image(bboxes, image):\n",
    "    images = []\n",
    "    data_list = []\n",
    "#     if len(bboxes) != 0:\n",
    "#     print(bboxes)\n",
    "    for pts in bboxes:\n",
    "        pts = pts.astype(np.float32)\n",
    "        data_list.append([pts[0][0], pts[0][1], pts[1][0], pts[2][1]])\n",
    "\n",
    "        rect = pts\n",
    "#         print(rect)\n",
    "        (top_left, top_right, bottom_right, bottom_left) = rect\n",
    "\n",
    "        w1 = abs(bottom_right[0] - bottom_left[0])\n",
    "        w2 = abs(top_right[0] - top_left[0])\n",
    "        h1 = abs(top_right[1] - bottom_right[1])\n",
    "        h2 = abs(top_left[1] - bottom_left[1])\n",
    "#         print(w1,w2,h1,h2)\n",
    "        max_width = max([w1, w2])\n",
    "        max_height = max([h1, h2])\n",
    "\n",
    "        dst = np.float32([[0, 0], [max_width - 1, 0], [max_width - 1, max_height - 1], [0, max_height - 1]])\n",
    "#         print(dst)\n",
    "        m = cv2.getPerspectiveTransform(rect, dst)\n",
    "        \n",
    "        warped = cv2.warpPerspective(image, m, (int(max_width), int(max_height)))\n",
    "        images.append(warped)\n",
    "\n",
    "    return images, data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a18594",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def add_argument(self, key, default=None, help=None,action=None,required=False,type=None):\n",
    "        key = key.replace('-','')\n",
    "        if action == 'store_true':\n",
    "            self.__dict__[key] = False\n",
    "        else:\n",
    "            self.__dict__[key] = default\n",
    "            \n",
    "        if required:\n",
    "            print(key,'/',help)\n",
    "            \n",
    "            \n",
    "    def set_argument(self, data_dict):\n",
    "        for key in data_dict:\n",
    "            self.__dict__[key] = data_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f6fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Args()\n",
    "\n",
    "parser.add_argument('--image_folder', required=True, help='path to image_folder which contains text images')\n",
    "parser.add_argument('--workers', type=int, help='number of data loading workers', default=4)\n",
    "parser.add_argument('--batch_size', type=int, default=192, help='input batch size')\n",
    "parser.add_argument('--saved_model', required=True, help=\"path to saved_model to evaluation\")\n",
    "\"\"\" Data processing \"\"\"\n",
    "parser.add_argument('--batch_max_length', type=int, default=25, help='maximum-label-length')\n",
    "parser.add_argument('--imgH', type=int, default=32, help='the height of the input image')\n",
    "parser.add_argument('--imgW', type=int, default=100, help='the width of the input image')\n",
    "parser.add_argument('--rgb', action='store_true', help='use rgb input')\n",
    "parser.add_argument('--character', type=str, default='0123456789abcdefghijklmnopqrstuvwxyz', help='character label')\n",
    "parser.add_argument('--sensitive', default=True, action='store_true', help='for sensitive character mode')\n",
    "parser.add_argument('--PAD', action='store_true', help='whether to keep ratio then pad for image resize')\n",
    "\"\"\" Model Architecture \"\"\"\n",
    "parser.add_argument('--Transformation', type=str, required=True, help='Transformation stage. None|TPS')\n",
    "parser.add_argument('--FeatureExtraction', type=str, required=True, help='FeatureExtraction stage. VGG|RCNN|ResNet')\n",
    "parser.add_argument('--SequenceModeling', type=str, required=True, help='SequenceModeling stage. None|BiLSTM')\n",
    "parser.add_argument('--Prediction', type=str, required=True, help='Prediction stage. CTC|Attn')\n",
    "parser.add_argument('--num_fiducial', type=int, default=20, help='number of fiducial points of TPS-STN')\n",
    "parser.add_argument('--input_channel', type=int, default=1, help='the number of input channel of Feature extractor')\n",
    "parser.add_argument('--output_channel', type=int, default=512,\n",
    "                        help='the number of output channel of Feature extractor')\n",
    "parser.add_argument('--hidden_size', type=int, default=256, help='the size of the LSTM hidden state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a004695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../opt.txt','r') as f:\n",
    "    opt = f.read()\n",
    "    \n",
    "opt = opt.split('------------ Options -------------\\n')\n",
    "opt = opt[-1].split('\\n')\n",
    "\n",
    "# test 실행용\n",
    "opt_dict ={}\n",
    "int_keys = ['manualSeed', 'workers', 'batch_size', 'num_iter', 'valInterval', 'batch_max_length', 'imgH', 'imgW', 'num_fiducial', 'input_channel', 'output_channel', 'hidden_size']\n",
    "float_keys = ['lr', 'beta1', 'rho', 'eps', 'grad_clip']\n",
    "bool_keys = ['FT', 'adam', 'rgb', 'sensitive', 'PAD', 'data_filtering_off']\n",
    "str_keys = ['exp_name', 'train_data', 'valid_data', 'saved_model', 'select_data', 'batch_ratio', 'total_data_usage_ratio', 'character', 'Transformation', 'FeatureExtraction', 'SequenceModeling', 'Prediction']\n",
    "save_keys = ['image_folder', 'workers', 'batch_size', 'saved_model', 'batch_max_length', 'imgH', 'imgW', 'rgb', 'character', 'sensitive', 'PAD', 'Transformation', 'FeatureExtraction', 'SequenceModeling', 'Prediction', 'num_fiducial', 'input_channel', 'output_channel', 'hidden_size','rgb']\n",
    "for i in opt:\n",
    "    t = i.split(':')\n",
    "    if i == '---------------------------------------':\n",
    "        break\n",
    "    else:\n",
    "        key, data = t[0].strip(), t[1][1:]\n",
    "        if key in int_keys:\n",
    "            data = int(data)\n",
    "        elif key in float_keys:\n",
    "            data = float(data)\n",
    "        elif key in bool_keys:\n",
    "            if data == 'True':\n",
    "                data = True\n",
    "            elif data == 'False':\n",
    "                data = False      \n",
    "        elif key in str_keys:\n",
    "            data = str(data)\n",
    "        if key in save_keys:\n",
    "            opt_dict[key] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d35e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model_name = '../best_accuracy.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b456ede6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser.set_argument(opt_dict)\n",
    "parser.sensitive = True\n",
    "# parser.character = ' 가각간갇갈갉갊감갑값갓갔강갖갗같갚갛개객갠갤갬갭갯갰갱갸갹갼걀걋걍걔걘걜거걱건걷걸걺검겁것겄겅겆겉겊겋게겐겔겜겝겟겠겡겨격겪견겯결겸겹겻겼경곁계곈곌곕곗고곡곤곧골곪곬곯곰곱곳공곶과곽관괄괆괌괍괏광괘괜괠괩괬괭괴괵괸괼굄굅굇굉교굔굘굡굣구국군굳굴굵굶굻굼굽굿궁궂궈궉권궐궜궝궤궷귀귁귄귈귐귑귓규균귤그극근귿글긁금급긋긍긔기긱긴긷길긺김깁깃깅깆깊까깍깎깐깔깖깜깝깟깠깡깥깨깩깬깰깸깹깻깼깽꺄꺅꺌꺼꺽꺾껀껄껌껍껏껐껑께껙껜껨껫껭껴껸껼꼇꼈꼍꼐꼬꼭꼰꼲꼴꼼꼽꼿꽁꽂꽃꽈꽉꽐꽜꽝꽤꽥꽹꾀꾄꾈꾐꾑꾕꾜꾸꾹꾼꿀꿇꿈꿉꿋꿍꿎꿔꿜꿨꿩꿰꿱꿴꿸뀀뀁뀄뀌뀐뀔뀜뀝뀨끄끅끈끊끌끎끓끔끕끗끙끝끼끽낀낄낌낍낏낑나낙낚난낟날낡낢남납낫났낭낮낯낱낳내낵낸낼냄냅냇냈냉냐냑냔냘냠냥너넉넋넌널넒넓넘넙넛넜넝넣네넥넨넬넴넵넷넸넹녀녁년녈념녑녔녕녘녜녠노녹논놀놂놈놉놋농높놓놔놘놜놨뇌뇐뇔뇜뇝뇟뇨뇩뇬뇰뇹뇻뇽누눅눈눋눌눔눕눗눙눠눴눼뉘뉜뉠뉨뉩뉴뉵뉼늄늅늉느늑는늘늙늚늠늡늣능늦늪늬늰늴니닉닌닐닒님닙닛닝닢다닥닦단닫달닭닮닯닳담답닷닸당닺닻닿대댁댄댈댐댑댓댔댕댜더덕덖던덛덜덞덟덤덥덧덩덫덮데덱덴델뎀뎁뎃뎄뎅뎌뎐뎔뎠뎡뎨뎬도독돈돋돌돎돐돔돕돗동돛돝돠돤돨돼됐되된될됨됩됫됴두둑둔둘둠둡둣둥둬뒀뒈뒝뒤뒨뒬뒵뒷뒹듀듄듈듐듕드득든듣들듦듬듭듯등듸디딕딘딛딜딤딥딧딨딩딪따딱딴딸땀땁땃땄땅땋때땍땐땔땜땝땟땠땡떠떡떤떨떪떫떰떱떳떴떵떻떼떽뗀뗄뗌뗍뗏뗐뗑뗘뗬또똑똔똘똥똬똴뙈뙤뙨뚜뚝뚠뚤뚫뚬뚱뛔뛰뛴뛸뜀뜁뜅뜨뜩뜬뜯뜰뜸뜹뜻띄띈띌띔띕띠띤띨띰띱띳띵라락란랄람랍랏랐랑랒랖랗래랙랜랠램랩랫랬랭랴략랸럇량러럭런럴럼럽럿렀렁렇레렉렌렐렘렙렛렝려력련렬렴렵렷렸령례롄롑롓로록론롤롬롭롯롱롸롼뢍뢨뢰뢴뢸룀룁룃룅료룐룔룝룟룡루룩룬룰룸룹룻룽뤄뤘뤠뤼뤽륀륄륌륏륑류륙륜률륨륩륫륭르륵른를름릅릇릉릊릍릎리릭린릴림립릿링마막만많맏말맑맒맘맙맛망맞맡맣매맥맨맬맴맵맷맸맹맺먀먁먈먕머먹먼멀멂멈멉멋멍멎멓메멕멘멜멤멥멧멨멩며멱면멸몃몄명몇몌모목몫몬몰몲몸몹못몽뫄뫈뫘뫙뫼묀묄묍묏묑묘묜묠묩묫무묵묶문묻물묽묾뭄뭅뭇뭉뭍뭏뭐뭔뭘뭡뭣뭬뮈뮌뮐뮤뮨뮬뮴뮷므믄믈믐믓미믹민믿밀밂밈밉밋밌밍및밑바박밖밗반받발밝밞밟밤밥밧방밭배백밴밸뱀뱁뱃뱄뱅뱉뱌뱍뱐뱝버벅번벋벌벎범법벗벙벚베벡벤벧벨벰벱벳벴벵벼벽변별볍볏볐병볕볘볜보복볶본볼봄봅봇봉봐봔봤봬뵀뵈뵉뵌뵐뵘뵙뵤뵨부북분붇불붉붊붐붑붓붕붙붚붜붤붰붸뷔뷕뷘뷜뷩뷰뷴뷸븀븃븅브븍븐블븜븝븟비빅빈빌빎빔빕빗빙빚빛빠빡빤빨빪빰빱빳빴빵빻빼빽뺀뺄뺌뺍뺏뺐뺑뺘뺙뺨뻐뻑뻔뻗뻘뻠뻣뻤뻥뻬뼁뼈뼉뼘뼙뼛뼜뼝뽀뽁뽄뽈뽐뽑뽕뾔뾰뿅뿌뿍뿐뿔뿜뿟뿡쀼쁑쁘쁜쁠쁨쁩삐삑삔삘삠삡삣삥사삭삯산삳살삵삶삼삽삿샀상샅새색샌샐샘샙샛샜생샤샥샨샬샴샵샷샹섀섄섈섐섕서석섞섟선섣설섦섧섬섭섯섰성섶세섹센셀셈셉셋셌셍셔셕션셜셤셥셧셨셩셰셴셸솅소속솎손솔솖솜솝솟송솥솨솩솬솰솽쇄쇈쇌쇔쇗쇘쇠쇤쇨쇰쇱쇳쇼쇽숀숄숌숍숏숑수숙순숟술숨숩숫숭숯숱숲숴쉈쉐쉑쉔쉘쉠쉥쉬쉭쉰쉴쉼쉽쉿슁슈슉슐슘슛슝스슥슨슬슭슴습슷승시식신싣실싫심십싯싱싶싸싹싻싼쌀쌈쌉쌌쌍쌓쌔쌕쌘쌜쌤쌥쌨쌩썅써썩썬썰썲썸썹썼썽쎄쎈쎌쏀쏘쏙쏜쏟쏠쏢쏨쏩쏭쏴쏵쏸쐈쐐쐤쐬쐰쐴쐼쐽쑈쑤쑥쑨쑬쑴쑵쑹쒀쒔쒜쒸쒼쓩쓰쓱쓴쓸쓺쓿씀씁씌씐씔씜씨씩씬씰씸씹씻씽아악안앉않알앍앎앓암압앗았앙앝앞애액앤앨앰앱앳앴앵야약얀얄얇얌얍얏양얕얗얘얜얠얩어억언얹얻얼얽얾엄업없엇었엉엊엌엎에엑엔엘엠엡엣엥여역엮연열엶엷염엽엾엿였영옅옆옇예옌옐옘옙옛옜오옥온올옭옮옰옳옴옵옷옹옻와왁완왈왐왑왓왔왕왜왝왠왬왯왱외왹왼욀욈욉욋욍요욕욘욜욤욥욧용우욱운울욹욺움웁웃웅워웍원월웜웝웠웡웨웩웬웰웸웹웽위윅윈윌윔윕윗윙유육윤율윰윱윳융윷으윽은을읊음읍읏응읒읓읔읕읖읗의읜읠읨읫이익인일읽읾잃임입잇있잉잊잎자작잔잖잗잘잚잠잡잣잤장잦재잭잰잴잼잽잿쟀쟁쟈쟉쟌쟎쟐쟘쟝쟤쟨쟬저적전절젊점접젓정젖제젝젠젤젬젭젯젱져젼졀졈졉졌졍졔조족존졸졺좀좁좃종좆좇좋좌좍좔좝좟좡좨좼좽죄죈죌죔죕죗죙죠죡죤죵주죽준줄줅줆줌줍줏중줘줬줴쥐쥑쥔쥘쥠쥡쥣쥬쥰쥴쥼즈즉즌즐즘즙즛증지직진짇질짊짐집짓징짖짙짚짜짝짠짢짤짧짬짭짯짰짱째짹짼쨀쨈쨉쨋쨌쨍쨔쨘쨩쩌쩍쩐쩔쩜쩝쩟쩠쩡쩨쩽쪄쪘쪼쪽쫀쫄쫌쫍쫏쫑쫓쫘쫙쫠쫬쫴쬈쬐쬔쬘쬠쬡쭁쭈쭉쭌쭐쭘쭙쭝쭤쭸쭹쮜쮸쯔쯤쯧쯩찌찍찐찔찜찝찡찢찧차착찬찮찰참찹찻찼창찾채책챈챌챔챕챗챘챙챠챤챦챨챰챵처척천철첨첩첫첬청체첵첸첼쳄쳅쳇쳉쳐쳔쳤쳬쳰촁초촉촌촐촘촙촛총촤촨촬촹최쵠쵤쵬쵭쵯쵱쵸춈추축춘출춤춥춧충춰췄췌췐취췬췰췸췹췻췽츄츈츌츔츙츠측츤츨츰츱츳층치칙친칟칠칡침칩칫칭카칵칸칼캄캅캇캉캐캑캔캘캠캡캣캤캥캬캭컁커컥컨컫컬컴컵컷컸컹케켁켄켈켐켑켓켕켜켠켤켬켭켯켰켱켸코콕콘콜콤콥콧콩콰콱콴콸쾀쾅쾌쾡쾨쾰쿄쿠쿡쿤쿨쿰쿱쿳쿵쿼퀀퀄퀑퀘퀭퀴퀵퀸퀼큄큅큇큉큐큔큘큠크큭큰클큼큽킁키킥킨킬킴킵킷킹타탁탄탈탉탐탑탓탔탕태택탠탤탬탭탯탰탱탸턍터턱턴털턺텀텁텃텄텅테텍텐텔템텝텟텡텨텬텼톄톈토톡톤톨톰톱톳통톺톼퇀퇘퇴퇸툇툉툐투툭툰툴툼툽툿퉁퉈퉜퉤튀튁튄튈튐튑튕튜튠튤튬튱트특튼튿틀틂틈틉틋틔틘틜틤틥티틱틴틸팀팁팃팅파팍팎판팔팖팜팝팟팠팡팥패팩팬팰팸팹팻팼팽퍄퍅퍼퍽펀펄펌펍펏펐펑페펙펜펠펨펩펫펭펴편펼폄폅폈평폐폘폡폣포폭폰폴폼폽폿퐁퐈퐝푀푄표푠푤푭푯푸푹푼푿풀풂품풉풋풍풔풩퓌퓐퓔퓜퓟퓨퓬퓰퓸퓻퓽프픈플픔픕픗피픽핀필핌핍핏핑하학한할핥함합핫항해핵핸핼햄햅햇했행햐향허헉헌헐헒험헙헛헝헤헥헨헬헴헵헷헹혀혁현혈혐협혓혔형혜혠혤혭호혹혼홀홅홈홉홋홍홑화확환활홧황홰홱홴횃횅회획횐횔횝횟횡효횬횰횹횻후훅훈훌훑훔훗훙훠훤훨훰훵훼훽휀휄휑휘휙휜휠휨휩휫휭휴휵휸휼흄흇흉흐흑흔흖흗흘흙흠흡흣흥흩희흰흴흼흽힁히힉힌힐힘힙힛힝!@#$%^&*《》()[]【】【】\\\"\\'◐◑oㅇ⊙○◎◉◀▶⇒◆■□△★※☎☏;:/.?<>-_=+×\\￦|₩~,.㎡㎥ℓ㎖㎘→「」『』·ㆍ1234567890abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ읩①②③④⑤月日軍 '\n",
    "parser.character = ' !\"%&\\'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\_abcdefghijklmnopqrstuvwxyz{}~→ㄱㄴㄹㅁㅅㅇㅔㅠㅣ가각간갇갈감갑값갓갔강갖같갚갛개객갤갬갯갱걀걔거걱건걷걸검겁것겅겉게겐겔겠겨격겪견결겸겹겼경곁계고곡곤곧골곰곱곳공곶과곽관괄괌광괘괜괭괴굉교구국군굳굴굵굶굽굿궁권궐궤귀귄귓규균귤그극근글긁금급긋긍기긴길김깁깃깅깆깊까깍깎깐깔깜깝깡깥깨깬깽꺠꺵꺼꺽꺾껌껍껏껑께껴꼬꼭꼴꼼꼽꼿꽁꽂꽃꽉꽝꽤꾀꾸꾹꾼꿀꿈꿍꿔꿨꿰뀌뀐뀝끄끈끊끌끓끔끗끙끝끼낄낌나낙낚난날낡남납낫났낭낮낯낱낳내낸낼냄냅냇냉냐냠냥너넉넌널넓넘넛넣네넥넨넬넷녀녁년념녕녘노녹논놀놈놉농높놓놔뇌뇨뇸누눈눌눔눕뉘뉴늄느늑는늘늙능늦늬니닉닌닐님닙닝다닥닦단닫달닭닮닳담답닷당닿대댁댄댐댓더덕던덜덟덤덥덧덩덫덮데덱덴델뎅뎌도독돈돌돔돕돗동돼됐되된될됨됩두둑둔둘둠둡둣둥뒤뒷듀드득든듣들듬듭듯등듸디딕딘딜딝딤딥딧딩딪따딱딴딸땀땅때땐땜땠땡떙떠떡떤떨떻떼또똑똘똥뚜뚝뚫뚱뛰뜀뜨뜩뜯뜰뜸뜻띄띠라락란랄람랍랐랑랗래랙랜램랫랬랭랲략량러럭런럴럼럽럿렀렁렇레렉렌렐렘렙렛려력련렬렴렵렷렸령례로록론롤롬롭롯롱뢰료룡루룩룬룰룸룹룻룽뤄뤼류륙륜률륭르륵른를름릅릇릉릎리릭린릴림립릿링마막만많말맑맘맙맛망맞맡맣매맥맨맵맹맺머먹먼멀멈멋멍멎메멘멜멤멩며면멸명몇모목몫몬몯몰몸몹못몽묘무묵묶문묻물뭄뭇뭉뭐뭔뭘뭣뮈뮌뮐뮤므믄믈미믹민믿밀밉밌밍및밑바박밖반받발밝밟밤밥방밭배백밴밸뱀뱃뱅뱉버벅번벌범법벗벙벚베벤벧벨벳벼벽변별볍병볕보복볶본볼봄봅봇봉봐봤뵈뵙부북분붇불붉붐붓붕붙뷔뷰브븐블븨비빅빈빌빔빗빙빚빛빠빡빨빱빵빼뺄뺏뺨뺴뻐뻔뻗뻬뼈뼉뼛뽀뽑뾰뿌뿍뿐뿔뿡쁘쁜쁨삐사삭산살삶삼삽삿샀상새색샌샐샘샛생샤샨샬샴샵샷샹섀서석섞선설섬섭섯성세섹센셀셈셉셋셔션셜셨셰셸소속손솔솜솝솟송솥쇄쇠쇼숍숏수숙순숟술숨숫숭숯숲숴쉐쉬쉰쉴쉼쉽쉿슈슐슛스슨슬슴습슷승시식신싣실싫심십싯싱싶싸싹싼쌀쌈쌌쌍쌓쌘쌤쌩써썩썬썰썹썼쎄쎈쎌쏘쏙쏟쑤쑥쓰쓴쓸씀씌씨씩씬씰씹씻씽아악안앉않알앓암압앗았앙앞애액앤앨앱앵야약얀얄얇양얕얗얘어억언얹얻얼엄업없엇었엉엊엌엎에엑엔엘엠엡엣엥여역엮연열엷염엽엿였영옆예옌옐옘옙옛오옥온올옮옳옵옷옹옻와왁완왓왔왕왜왠외왼요욕욘욜욤용우욱운울움웃웅워원월웠웨웩웬웰웹위윅윈윌윗윙유육윤율융윷으윽은을음읍응읗의이익인일읽잃임입잇있잉잊잎자작잔잖잘잠잡잣장잦재잭잰잼쟁쟤저적전절젊젋점접젓정젖제젝젠젤젯젱져졌조족존졸좀좁종좋좌죄죠주죽준줄줌줍중줘쥐쥘쥬쥴즈즉즌즐즘즙증지직진질짐집짓징짙짚짜짝짠짧짱째쨌쨰쩌쩍쩐쩔쩜쩰쪄쪽쫑쫓쬘쭈쭉쯔쯤찌찍찔찜찡찢찧차착찬찮찰참찻창찾채책챈챌챔챙챠처척천철첨첩첫청체첵첸첼쳐쳤초촉촌촘촛총촬최추축춘출춤춥춧충춰췌취츠측츰층치칙친칠침칩칫칭카칵칸칼캄캅캉캐캔캘캠캡캣캥커컥컨컬컴컵컷케켄켈켓켜켰코콕콘콜콤콥콧콩콰콸쾅쾌쿄쿠쿡쿤쿨쿵쿼퀀퀘퀴퀵퀸큐큘크큰클큼키킥킨킬킴킵킷킹타탁탄탈탉탐탑탓탕태택탠탤탬탭탱터턱턴털텀텃텅테텍텐텔템텝텨톈토톡톤톨톰톱톳통톺퇴투툭툰툴툼퉁튀튜트특튼튿틀틈티틱틴틸팀팁팃팅파팎판팔팜팝팟팡팥패팩팬팰팻팽퍼퍽펀펄펌페펙펜펠펩펫펭펴편펼평폐포폭폰폴폼퐁표푸푹푼풀품풋풍퓨퓰프픈플픔피픽핀필핌핏핑하학한할함합핫항핳해핵핸햄햇했행햐향허헌헐험헛헤헨헬헴헸혀혁현혈혐협혔형혜호혹혼홀홈홉홍화확환활황회획횐횟횡효후훈훌훔훤훨훼휀휘휙휠휴흄흉흐흑흔흘흙흠흡흥흩희흰히힌힐힘힝，．０１ＥＨＰＲＴａｋｌｍ'\n",
    "parser.image_folder = 'deep-text-recognition-benchmark/demo_image'\n",
    "# parser.saved_model = save_model_name\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = True\n",
    "\n",
    "opt = parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02861722",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_load_befo_mem = torch.cuda.memory_allocated()/1024/1024\n",
    "print(\"current_memory:\", rec_load_befo_mem)\n",
    "\n",
    "from utill import CTCLabelConverter, AttnLabelConverter\n",
    "from ocrmodel import Model\n",
    "from dataset import AlignCollate, RawDataset\n",
    "\n",
    "#boxes, polys, ret_score_text\n",
    "\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "images, _ = cut_image(boxes, gray)\n",
    "\n",
    "# parser.image_folder = images\n",
    "# converter = AttnLabelConverter(opt.character)\n",
    "converter = CTCLabelConverter(opt.character)\n",
    "opt.num_class = len(converter.character)\n",
    "# if opt.rgb:\n",
    "#     opt.input_channel = 3\n",
    "\n",
    "# model = Model(opt)\n",
    "# model.load_state_dict(torch.load(opt.saved_model, map_location=device),strict=False)\n",
    "# model.to(device)\n",
    "\n",
    "# model.eval()\n",
    "\n",
    "# rec_load_mem = torch.cuda.memory_allocated()/1024/1024\n",
    "# print(\"trt_load_mem: %fMB\"%(rec_load_mem-rec_load_befo_mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba5901",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61cfeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b824d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "AlignCollate_demo = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
    "demo_data = RawDataset(root=images, opt=opt)  # use RawDataset\n",
    "\n",
    "#     if device=='cuda':\n",
    "demo_loader = torch.utils.data.DataLoader(\n",
    "    demo_data, batch_size=opt.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=int(opt.workers),\n",
    "    collate_fn=AlignCollate_demo, pin_memory=True)\n",
    "\n",
    "# with torch.no_grad():\n",
    "for image_tensors, image_path_list in demo_loader:\n",
    "    batch_size = image_tensors.size(0)\n",
    "    image = image_tensors.to(device)\n",
    "\n",
    "    # For max length prediction\n",
    "    length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "#     text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e11d9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.saved_model = \"./deep-text-recognition-benchmark/best_accuracy_noparell.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a50929",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(opt)\n",
    "print('model input parameters', opt.imgH, opt.imgW, opt.num_fiducial, opt.input_channel, opt.output_channel,\n",
    "      opt.hidden_size, opt.num_class, opt.batch_max_length, opt.Transformation, opt.FeatureExtraction,\n",
    "      opt.SequenceModeling, opt.Prediction)\n",
    "# model = torch.nn.DataParallel(model).to(device)\n",
    "# model2 = model.module.to(device)\n",
    "# load model\n",
    "print('loading pretrained model from %s' % opt.saved_model)\n",
    "model.load_state_dict(torch.load(opt.saved_model, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f69fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt.Transformation = 'TPS' \n",
    "opt.Transformation = 'None' \n",
    "opt.FeatureExtraction = 'ResNet' \n",
    "opt.SequenceModeling = 'BiLSTM' \n",
    "opt.Prediction = 'CTC'\n",
    "# opt.Prediction = 'Attn'\n",
    "# opt.image_folder = 'demo_image/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405df4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c = Model(opt)\n",
    "\n",
    "model_c.FeatureExtraction = model.FeatureExtraction\n",
    "\n",
    "model_c.AdaptiveAvgPool = model.AdaptiveAvgPool\n",
    "model_c.SequenceModeling = model.SequenceModeling\n",
    "model_c.Prediction = model.Prediction.generator\n",
    "\n",
    "# prepare data. two demo images from https://github.com/bgshih/crnn#run-demo\n",
    "# AlignCollate_demo = AlignCollate(imgH=opt.imgH, imgW=opt.imgW, keep_ratio_with_pad=opt.PAD)\n",
    "# demo_data = RawDataset(root=opt.image_folder, opt=opt)  # use RawDataset\n",
    "# demo_loader = torch.utils.data.DataLoader(\n",
    "#     demo_data, batch_size=opt.batch_size,\n",
    "#     shuffle=False,\n",
    "#     num_workers=int(opt.workers),\n",
    "#     collate_fn=AlignCollate_demo, pin_memory=True)\n",
    "\n",
    "# predict\n",
    "model_c.eval()\n",
    "with torch.no_grad():\n",
    "    for image_tensors, image_path_list in demo_loader:\n",
    "        batch_size = image_tensors.size(0)\n",
    "        image = image_tensors.to(device)\n",
    "        # For max length prediction\n",
    "        length_for_pred = torch.IntTensor([opt.batch_max_length] * batch_size).to(device)\n",
    "        text_for_pred = torch.LongTensor(batch_size, opt.batch_max_length + 1).fill_(0).to(device)\n",
    "\n",
    "        if 'CTC' in opt.Prediction:\n",
    "            preds = model_c(image, text_for_pred)\n",
    "\n",
    "            # Select max probabilty (greedy decoding) then decode index to character\n",
    "            preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "            _, preds_index = preds.max(2)\n",
    "            # preds_index = preds_index.view(-1)\n",
    "            preds_str = converter.decode(preds_index, preds_size)\n",
    "\n",
    "        else:\n",
    "            preds = model_c(image, text_for_pred, is_train=False)\n",
    "\n",
    "            # select max probabilty (greedy decoding) then decode index to character\n",
    "            _, preds_index = preds.max(2)\n",
    "            preds_str = converter.decode(preds_index, length_for_pred)\n",
    "\n",
    "\n",
    "        log = open(f'./log_demo_result.txt', 'a')\n",
    "        dashed_line = '-' * 80\n",
    "        head = f'{\"image_path\":25s}\\t{\"predicted_labels\":25s}\\tconfidence score'\n",
    "\n",
    "        print(f'{dashed_line}\\n{head}\\n{dashed_line}')\n",
    "        log.write(f'{dashed_line}\\n{head}\\n{dashed_line}\\n')\n",
    "\n",
    "        preds_prob = F.softmax(preds, dim=2)\n",
    "        preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "        for img_name, pred, pred_max_prob in zip(image_path_list, preds_str, preds_max_prob):\n",
    "            if 'Attn' in opt.Prediction:\n",
    "                pred_EOS = pred.find('[s]')\n",
    "                pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "                pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "            # calculate confidence score (= multiply of pred_max_prob)\n",
    "            confidence_score = pred_max_prob.cumprod(dim=0)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d451d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "\n",
    "\n",
    "\n",
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "\n",
    "\n",
    "class TrtModel:\n",
    "    \n",
    "    def __init__(self,engine_path,max_batch_size=1,dtype=np.float32, batch_size=1):\n",
    "        \n",
    "        self.engine_path = engine_path\n",
    "        self.dtype = dtype\n",
    "        self.logger = trt.Logger(trt.Logger.WARNING)\n",
    "#         self.logger = trt.Logger()\n",
    "        self.batch_size = batch_size\n",
    "        self.runtime = trt.Runtime(self.logger)\n",
    "        self.engine = self.load_engine(self.runtime, self.engine_path)\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.inputs, self.outputs, self.bindings, self.stream = self.allocate_buffers()\n",
    "        self.context = self.engine.create_execution_context()\n",
    "                \n",
    "    \n",
    "    @staticmethod\n",
    "    def load_engine(trt_runtime, engine_path):\n",
    "        trt.init_libnvinfer_plugins(None, \"\")             \n",
    "        with open(engine_path, 'rb') as f:\n",
    "            engine_data = f.read()\n",
    "        engine = trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "        return engine\n",
    "    \n",
    "    def allocate_buffers(self):\n",
    "        \n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        bindings = []\n",
    "        stream = cuda.Stream()\n",
    "        \n",
    "        for binding in self.engine:\n",
    "            dims = self.engine.get_binding_shape(binding)\n",
    "            dims[0] = self.batch_size\n",
    "            \n",
    "            size = trt.volume(dims)\n",
    "            dtype = trt.nptype(self.engine.get_binding_dtype(binding))\n",
    "            \n",
    "            host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "            device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "            \n",
    "            bindings.append(int(device_mem))\n",
    "\n",
    "            if self.engine.binding_is_input(binding):\n",
    "                inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "            else:\n",
    "                outputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "        \n",
    "        return inputs, outputs, bindings, stream\n",
    "        \n",
    "        \n",
    "    def __call__(self, x:np.ndarray):\n",
    "        \n",
    "        x = x.astype(self.dtype)\n",
    "#         batch_size = x.shape[0]\n",
    "#         print(batch_size)\n",
    "#         self.inputs[0].host = self.inputs[0].host * batch_size\n",
    "#         print(self.inputs[0].host.shape, x.ravel().shape)\n",
    "\n",
    "        np.copyto(self.inputs[0].host,x.ravel()) # 두번쨰 인자를 첫번쨰 인자에다 복사\n",
    "        \n",
    "        self.context.set_binding_shape(0, x.shape)\n",
    "        \n",
    "        for inp in self.inputs:\n",
    "            cuda.memcpy_htod_async(inp.device, inp.host, self.stream)\n",
    "        \n",
    "        self.context.execute_async(batch_size=self.batch_size, bindings=self.bindings, stream_handle=self.stream.handle)\n",
    "        \n",
    "        for out in self.outputs:\n",
    "            cuda.memcpy_dtoh_async(out.host, out.device, self.stream) \n",
    "        \n",
    "        self.stream.synchronize()\n",
    "        \n",
    "        bs = self.context.get_binding_shape(1)\n",
    "        \n",
    "        y_out = self.outputs[-1].host\n",
    "\n",
    "        y1 =  y_out[0:np.array(bs).prod()].reshape(bs)\n",
    "        \n",
    "        return y1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d252e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(image.detach().cpu())\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff44598",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_engine_path = './deep-text-recognition-benchmark/weights/ocr.engine'\n",
    "batch_size = data.shape[0]\n",
    "model = TrtModel(trt_engine_path, batch_size=batch_size)\n",
    "shape = model.engine.get_binding_shape(0) # shape 0번째는 인풋 1 의 shape, 1번쨰는 아웃풋 1의 shape\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ca2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.engine.get_binding_shape(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75759b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cffbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.tensor(result)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5877027",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "_, preds_index = preds.max(2)\n",
    "preds_index = preds_index.view(-1)\n",
    "preds_str = converter.decode(preds_index.data, preds_size.data)\n",
    "preds_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653feb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "_, preds_index = preds.max(2)\n",
    "preds_index = preds_index.view(-1)\n",
    "\n",
    "preds_str = []\n",
    "for index, l in enumerate(preds_size.data):\n",
    "\n",
    "    t = preds_index.data[index:]\n",
    "    \n",
    "    char_list = []\n",
    "    for i in range(l):\n",
    "        if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank.\n",
    "            char_list.append(opt.character[t[i]])\n",
    "    text = ''.join(char_list)\n",
    "\n",
    "    preds_str.append(text)\n",
    "\n",
    "\n",
    "preds_prob = F.softmax(preds, dim=2)\n",
    "preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "\n",
    "result_list = []\n",
    "for pred, pred_max_prob in zip(preds_str, preds_max_prob):\n",
    "#     if 'Attn' in opt.Prediction:\n",
    "#         pred_EOS = pred.find('[s]')\n",
    "#         pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "#         pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "    # calculate confidence score (= multiply of pred_max_prob)\n",
    "    try:\n",
    "        confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "    except:\n",
    "        confidence_score  = 0\n",
    "    result_list.append([pred, float(confidence_score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f977e2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb25faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "\n",
    "TRT_LOGGER = trt.Logger()\n",
    "\n",
    "with open('./deep-text-recognition-benchmark/weights/ocr.engine', \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "    engine =  runtime.deserialize_cuda_engine(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb56683",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HostDeviceMem(object):\n",
    "    def __init__(self, host_mem, device_mem):\n",
    "        self.host = host_mem\n",
    "        self.device = device_mem\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Host:\\n\" + str(self.host) + \"\\nDevice:\\n\" + str(self.device)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b1071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "outputs = []\n",
    "bindings = []\n",
    "\n",
    "for binding in engine:\n",
    "    dims = engine.get_binding_shape(binding)\n",
    "    dims[0] = batch_size\n",
    "#     if dims[-1] == -1:\n",
    "#         dims[-2],dims[-1] = input_shape\n",
    "#     else:\n",
    "#         dims[-3],dims[-2] = int(input_shape[0]/2), int(input_shape[1]/2)\n",
    "        \n",
    "    size = trt.volume(dims) * engine.max_batch_size\n",
    "    dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "    \n",
    "    host_mem = cuda.pagelocked_empty(size, dtype)\n",
    "    device_mem = cuda.mem_alloc(host_mem.nbytes)\n",
    "    \n",
    "    bindings.append(int(device_mem))\n",
    "    \n",
    "    if engine.binding_is_input(binding):#Determine whether a binding is an input binding.\n",
    "        inputs.append(HostDeviceMem(host_mem, device_mem))\n",
    "    else:\n",
    "        outputs.append(HostDeviceMem(host_mem, device_mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eade7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_inputs, segment_outputs, segment_bindings = inputs, outputs, bindings\n",
    "\n",
    "stream = cuda.Stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with engine.create_execution_context() as context:\n",
    "context = engine.create_execution_context()\n",
    "context.active_optimization_profile = 0\n",
    "origin_inputshape=context.get_binding_shape(0)\n",
    "\n",
    "if (origin_inputshape[-1]==-1):\n",
    "    origin_inputshape[-2], origin_inputshape[-1]=(input_shape)\n",
    "    context.set_binding_shape(0,(origin_inputshape))\n",
    "\n",
    "else:\n",
    "    origin_inputshape[0] = batch_size\n",
    "    context.set_binding_shape(0,(origin_inputshape))\n",
    "\n",
    "# input_img_array = np.array(image * engine.max_batch_size)\n",
    "input_img_array = np.array(image.detach().cpu() * engine.max_batch_size)\n",
    "# print(input_img_array.shape)\n",
    "# img = torch.from_numpy(input_img_array).float()\n",
    "\n",
    "segment_inputs[0].host = input_img_array\n",
    "\n",
    "[cuda.memcpy_htod_async(inp.device, inp.host, stream) for inp in segment_inputs] \n",
    "stream.synchronize()\n",
    "\n",
    "context.execute_async_v2(bindings=segment_bindings, stream_handle=stream.handle)\n",
    "stream.synchronize()\n",
    "\n",
    "[cuda.memcpy_dtoh_async(out.host, out.device, stream) for out in segment_outputs]\n",
    "stream.synchronize()\n",
    "\n",
    "bs = context.get_binding_shape(1)\n",
    "\n",
    "y_out = segment_outputs[-1].host\n",
    "\n",
    "y1 =  y_out[0:np.array(bs).prod()].reshape(bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589231c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.tensor(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee70ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torch.tensor(y1) * 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfbe554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "preds_size = torch.IntTensor([preds.size(1)] * batch_size)\n",
    "_, preds_index = preds.max(2)\n",
    "preds_index = preds_index.view(-1)\n",
    "\n",
    "preds_str = []\n",
    "for index, l in enumerate(preds_size.data):\n",
    "\n",
    "    t = preds_index.data[index:]\n",
    "\n",
    "    char_list = []\n",
    "    for i in range(l):\n",
    "        if t[i] != 0 and (not (i > 0 and t[i - 1] == t[i])):  # removing repeated characters and blank.\n",
    "            char_list.append(opt.character[t[i]])\n",
    "    text = ''.join(char_list)\n",
    "\n",
    "    preds_str.append(text)\n",
    "\n",
    "\n",
    "preds_prob = F.softmax(preds, dim=2)\n",
    "preds_max_prob, _ = preds_prob.max(dim=2)\n",
    "\n",
    "result_list = []\n",
    "for pred, pred_max_prob in zip(preds_str, preds_max_prob):\n",
    "#     if 'Attn' in opt.Prediction:\n",
    "#         pred_EOS = pred.find('[s]')\n",
    "#         pred = pred[:pred_EOS]  # prune after \"end of sentence\" token ([s])\n",
    "#         pred_max_prob = pred_max_prob[:pred_EOS]\n",
    "\n",
    "    # calculate confidence score (= multiply of pred_max_prob)\n",
    "    try:\n",
    "        confidence_score = pred_max_prob.cumprod(dim=0)[-1]\n",
    "    except:\n",
    "        confidence_score  = 0\n",
    "    result_list.append([pred, float(confidence_score)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5743d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4634ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be30eee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
