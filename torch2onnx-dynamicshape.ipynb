{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da493a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "os.environ['NVIDIA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "print('Count of using GPUs:', torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d067213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import onnx\n",
    "import numpy as np\n",
    "import argparse\n",
    "import onnx_graphsurgeon as gs\n",
    "\n",
    "from utils import experiment_loader, initial_logger, copyStateDict, get_cfg_defaults\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from craft import CRAFT\n",
    "\n",
    "cfg_detec = get_cfg_defaults()\n",
    "cfg_detec\n",
    "# weight_path = '../weights'\n",
    "# model_path, model_config = experiment_loader(model_format='pth', data_path=weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f6288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args():\n",
    "    def __init__(self,volume_path='../data/Synapse/test_vol_h5',dataset='Synapse',num_classes=9,img_size=224,vit_name='R50-ViT-B_16',n_skip=3,vit_patches_size=16):\n",
    "        self.volume_path = volume_path\n",
    "        self.dataset = dataset\n",
    "        self.num_classes = num_classes\n",
    "        self.img_size = img_size\n",
    "        self.vit_name = vit_name\n",
    "        self.n_skip = n_skip\n",
    "        self.vit_patches_size = vit_patches_size\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bf08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "_____________________________________________________________________________\n",
    "\n",
    "This file contain code for converting trained model into ONNX format\n",
    "Refer from TensorRT example: tensorrt/bin/python/onnx_packnet\n",
    "_____________________________________________________________________________\n",
    "\"\"\"\n",
    "# from icecream import ic\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import onnx\n",
    "import numpy as np\n",
    "import argparse\n",
    "import onnx_graphsurgeon as gs\n",
    "\n",
    "from utils import experiment_loader, initial_logger, copyStateDict, get_cfg_defaults\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from craft import CRAFT\n",
    "\n",
    "logger = initial_logger()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def build_onnx(weight_path):\n",
    "    \"\"\"Load the network and export it to ONNX\n",
    "    \"\"\"\n",
    "    logger.info(\"Converting detec pth to onnx...\")\n",
    "\n",
    "    model_path, model_config = experiment_loader(model_format='pth', data_path=weight_path)\n",
    "    \n",
    "    # Load config come with trained model\n",
    "    cfg_detec = get_cfg_defaults()\n",
    "    cfg_detec.merge_from_file(model_config)\n",
    "    \n",
    "    # Set output path for onnx files\n",
    "    output_path = Path('../weights')\n",
    "    \n",
    "    # Set name for onnx files\n",
    "    output_detec = os.path.join(output_path, \"detect.onnx\")\n",
    "    \n",
    "    # Dummy input data for models\n",
    "    input_tensor_detec = torch.randn((1, 3, 500, 500), requires_grad=False)\n",
    "    input_tensor_detec=input_tensor_detec.cuda()\n",
    "    input_tensor_detec=input_tensor_detec.to(device=device)\n",
    "    \n",
    "    # Load net\n",
    "    net = CRAFT()\n",
    "    net.load_state_dict(copyStateDict(torch.load(model_path)))\n",
    "    net = net.to(device)\n",
    "    net.eval()\n",
    "    \n",
    "    # Convert the model into ONNX\n",
    "    torch.onnx.export(net, input_tensor_detec, output_detec,\n",
    "                      verbose=True, opset_version=cfg_detec.INFERENCE.OX_OPSET,\n",
    "                      do_constant_folding=cfg_detec.INFERENCE.OX_DO_CONSTANT_FOLDING,\n",
    "                      export_params=cfg_detec.INFERENCE.OX_EXPORT_PARAMS,\n",
    "                      input_names=[\"input\"],\n",
    "                      output_names=[\"output\"],\n",
    "                      dynamic_axes={\"input\": {2: \"height\", 3: \"width\"}})\n",
    "#     torch.onnx.export(net, input_tensor_detec, output_detec,\n",
    "#                       verbose=True, opset_version=cfg_detec.INFERENCE.OX_OPSET,\n",
    "#                       do_constant_folding=cfg_detec.INFERENCE.OX_DO_CONSTANT_FOLDING,\n",
    "#                       export_params=cfg_detec.INFERENCE.OX_EXPORT_PARAMS,\n",
    "#                       input_names=[\"input\"],\n",
    "#                       output_names=[\"output\", \"output1\"],\n",
    "#                       dynamic_axes={\"input\": {0: \"batch\", 2: \"height\", 3: \"width\"}})\n",
    "\n",
    "    logger.info(\"Convert detec pth to ONNX sucess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser = argparse.ArgumentParser(description=\"Exports model to ONNX, and post-processes it to insert TensorRT plugins\")\n",
    "# parser.add_argument(\"--weight\", required=False, help=\"Path to input model folder\", default='/workspace/DBP/NAS저장공간/hengbee/ONNX-TensorRT-Inference-CRAFT-pytorch/weights')\n",
    "# args=parser.parse_args()\n",
    "# print(args.weight)\n",
    "\n",
    "weight_path = '../weights'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d81e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_onnx(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f658599d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451ae35c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95140dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f19676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import time\n",
    "\n",
    "onnx_load_befo_mem = torch.cuda.memory_allocated()/1024/1024\n",
    "print(\"current_memory:\", onnx_load_befo_mem)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "output_path = Path('../weights')\n",
    "# Set name for onnx files\n",
    "output_detec = os.path.join(output_path, \"transunet.onnx\")\n",
    "# Load the ONNX model\n",
    "model = onnx.load(output_detec)\n",
    "\n",
    "# Check that the model is well formed\n",
    "onnx.checker.check_model(model)\n",
    "\n",
    "print(f'load time: {time.time()-t2}')\n",
    "\n",
    "onnx_load_mem = torch.cuda.memory_allocated()/1024/1024\n",
    "\n",
    "print(\"onnx_load_mem: %fMB\"%(onnx_load_mem-onnx_load_befo_mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6a8b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print a human readable representation of the graph\n",
    "print(onnx.helper.printable_graph(model.graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68e0220",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n",
    "import imgproc\n",
    "import cv2\n",
    "from torch.autograd import Variable\n",
    "\n",
    "onnx_infer_befo_mem = torch.cuda.memory_allocated()/1024/1024\n",
    "print(\"current_memory:\", onnx_infer_befo_mem)\n",
    "\n",
    "t = time.time()\n",
    "image_path = '../IMG_8178.jpg'\n",
    "print(\"Test image :\", image_path)\n",
    "\n",
    "# image = imgproc.loadImage(image_path)\n",
    "ort_session = ort.InferenceSession(output_detec)\n",
    "# img_resized = imgproc.normalizeMeanVariance(image)\n",
    "# img_resized = torch.from_numpy(img_resized).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n",
    "# img_resized = Variable(img_resized.unsqueeze(0))\n",
    "# img_resized.cuda()\n",
    "# print(img_resized.shape)\n",
    "img = cv2.imread('../images.jpeg',cv2.IMREAD_COLOR)\n",
    "dsize=(224,224)\n",
    "img_resized = cv2.resize(img, dsize)\n",
    "img_resized = torch.from_numpy(img_resized).permute(2, 0, 1)    # [h, w, c] to [c, h, w]\n",
    "img_resized = Variable(img_resized.unsqueeze(0))\n",
    "img_resized.cuda()\n",
    "# print(img_resized.shape)\n",
    "outputs = ort_session.run(\n",
    "    None,\n",
    "    {\"input\": np.array(img_resized).astype(np.float32)},\n",
    ")\n",
    "\n",
    "print(f'load time: {time.time()-t}')\n",
    "onnx_infer_mem = torch.cuda.memory_allocated()/1024/1024\n",
    "\n",
    "print(\"onnx_load_mem: %fMB\"%(onnx_infer_mem-onnx_infer_befo_mem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8814bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as pl\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "cmap=pl.cm.tab10_r\n",
    "\n",
    "my_cmap = cmap(np.arange(cmap.N))\n",
    "my_cmap[:, -1] = 0.9\n",
    "my_cmap[0, -1] = 0.1\n",
    "my_cmap = ListedColormap(my_cmap)\n",
    "print(my_cmap.colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e7dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class = np.argmax(outputs[0], axis=-1)\n",
    "y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74867a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_seg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f328543",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sc in score_seg:\n",
    "    alpha=0.8\n",
    "    plt.imshow(sc, cmap=my_cmap, alpha=alpha)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63226d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_text = outputs[0][0,:,:,0]\n",
    "score_link = outputs[0][0,:,:,1]\n",
    "score_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ae010",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd7547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def img_show(img, size =(15,15)):\n",
    "    plt.rcParams[\"figure.figsize\"] = size\n",
    "    imgplot = plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b73ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_show(score_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233ba377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
